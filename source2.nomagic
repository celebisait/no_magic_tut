# HTML
{}
<center><h1>Part 2: Softmax Regression <span style="color: red">[Draft]</span></h1></center>

<h1>Introduction</h1>
<p>
Let's say we want to build a model to discriminate the following <b>red</b>, <b>blue</b>
and <b>green</b> points in 2-dimensional space:
</p>

# CODE
{'type': 'image', 'width': 600}
import numpy as np
import matplotlib.pyplot as plt

X = np.array([[-0.1, -0.5, 1.3, -0.6, -1.5, 0.2, -0.3,  0.7,  1.1, -1.0,
               -0.5, -1.3,  -1.4, -0.9, 0.4 , -0.4, 0.3, -1.6, -0.5, -1.0],
              [1.4,  -0.1, 0.9,  0.4,  0.4, 0.2, -0.4, -0.8, -1.5,  0.9,
               1.5, -0.45, -1.2, -0.7, -1.3, 0.6, -0.5, -0.7, -1.4, -1.4]])
Y = np.array([[0, 0, 1, 0, 2, 1, 1, 1, 1, 0, 0, 2, 2, 2, 1, 0, 1, 2, 2, 2]])
colormap = np.array(['r', 'b', 'g'])

def plot_scatter(X, Y, colormap, path):
   plt.grid()
   plt.xlim([-2.0, 2.0])
   plt.ylim([-2.0, 2.0])
   plt.xlabel('$x_1$', size=20)
   plt.ylabel('$x_2$', size=20)
   plt.title('Input 2D points', size=18)
   plt.scatter(X[0], X[1], s=50, c=colormap[Y[0]])
   plt.savefig(path)

plot_scatter(X, Y, colormap, 'image.png')

plt.close()
plt.clf()
plt.cla()

# HTML
{}
<p>
In other words, given a point, $(x_1, x_2)$, we want to output either <b>red</b>, <b>blue</b> or <b>green</b>.
</p>

<p>
We can use <b>Softmax Regression</b> for this problem. We first learn <b>weights</b>
($w_{1,1}, w_{1,2}, w_{2,1}, w_{2,2}, w_{3,1}, w_{3,2}$) and <b>bias</b> ($b_1, b_2, b_3$).
This phase is called <b>training</b>. Then we use the following formula to predict if the
new point is red, blue or green. This phase is called <b>prediction</b> or <b>inference</b>.
</p>

<h1>One hot vector representation</h1>

<p>
We represent the output as a one hot vector. In other words, we represent <b>red points</b>
using $\begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}$ and similarly for <b>blue points</b> using
$\begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}$ and lastly for <b>green points</b> using
$\begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}$.
</p>

<h1>Computation Graph</h1>

<p>
Here is a visual representation of our model:
</p>

<img class="static_image" style="width: 900px;" src="static_images/image003.png"/>

<p>
and simply pick the biggest $a_i$ to do the <b>final prediction</b>.
</p>

<p>
We use sigmoid function as $g(z)$:
</p>

$$ g(z) = \frac{1}{1+e^{-z}} $$

<h1>Maximum Likelihood Estimation</h1>

<p>
In training, our goal is to <b>learn</b> a matrix $W$ of size $(3 \times 2)$  and a $b$ of size $(3 \times 1)$
that best <b>discriminates</b> red, blue and green points.
</p>

<p>
We want to find $W$ and $b$ that minimizes some definition of a <b>cost function</b>.
Let's attempt to write a cost function for this problem.
</p>

<p>
Let's say we have three points:
</p>

$$x = \begin{bmatrix} -0.1 \\ 1.4 \end{bmatrix}, y=0$$

$$x = \begin{bmatrix} 1.3  \\ 0.9 \end{bmatrix}, y=1$$

<p>
and similarly,
</p>

$$x = \begin{bmatrix} -1.4  \\ -1.1 \end{bmatrix}, y=2$$

<p>
Now, let's list these $y$ as a <b>one hot vector</b> and, their corresponding $a$ values:
</p>

$$y = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, a = \begin{bmatrix} 0.9 \\ 0.1 \\ 0.0 \end{bmatrix} $$

$$y = \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}, a = \begin{bmatrix} 0.1 \\ 0.8 \\ 0.1 \end{bmatrix} $$

$$y = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}, a = \begin{bmatrix} 0.1 \\ 0.2 \\ 0.7 \end{bmatrix} $$

<p>
Intuitively, we want a classifier that produces <b>similar</b> looking $a$ and $y$. This means, if
$y = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}$, then, for example, having
$a = \begin{bmatrix} 0.8 \\ 0.1 \\ 0.1 \end{bmatrix}$ is <b>more desirable</b> than having
$a = \begin{bmatrix} 0.6 \\ 0.2 \\ 0.2 \end{bmatrix}$.

<p>
In other words, we want to <b>maximize</b>:
</p>

$$P(y|x) = \prod_{j=1}^{3} {a_j^{y_j} (1-a_j)^{(1-y_j)} }$$

<p>
Here, $a_j$ represents the jth item in the vector $a$, and similarly $y_j$ represents the jth value in $y$.
For example, when $a = \begin{bmatrix} 0.9 \\ 0.1 \\ 0.0 \end{bmatrix}$, then, $a_1 = 0.9, a_2 = 0.1$ and
$a_3 = 0.0$.
</p>

$$y = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, a = \begin{bmatrix} 0.9 \\ 0.1 \\ 0.0 \end{bmatrix}, P(y|x) = 0.9 \times 0.9 \times 1.0 = 0.810 $$

$$y = \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}, a = \begin{bmatrix} 0.1 \\ 0.8 \\ 0.1 \end{bmatrix}, P(y|x) = 0.9 \times 0.8 \times 0.9 = 0.648 $$

$$y = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}, a = \begin{bmatrix} 0.1 \\ 0.2 \\ 0.7 \end{bmatrix}, P(y|x) = 0.9 \times 0.8 \times 0.7 = 0.504 $$

<p>
<b>Bigger</b> the $P(y|x)$ is the <b>better</b>.
</p>

<p>
Similar to <b>Logistic Regression</b>, in order to define the <b>loss</b> for multiple samples, we will simply multiply
each value for each sample (<b>Maximum Likelihood Estimation</b>):
</p>

$$J = \prod_{i=1}^{M} P(y^{(i)}|x^{(i)}) $$

<p>
here, $y^{(i)}$, $x^{(i)}$ and $a^{(i)}$ corresponds to ith sample in the training set out of $M$ training samples. We can rewrite it as:
</p>

$$J = \prod_{i=1}^{M} \prod_{j=1}^{3}  (a_j^{(i)}) ^ {y_j^{(i)}} (1-a_j^{(i)})^{(1-y_j^{(i)})} $$

<p>
Maximizing above is equal to maximizing:
</p>

$$J = log \left( \prod_{i=1}^{M} \prod_{j=1}^{3} (a_j^{(i)}) ^ {y_j^{(i)}} (1-a_j^{(i)})^{(1-y_j^{(i)})} \right ) $$

<p>
with more math:
</p>

$$J = \sum_{i=1}^{M} \sum_{j=1}^{3} y_j^{(i)} log(a_j^{(i)}) (1-y_j^{(i)}) log(1-a_j^{(i)}) $$

<p>
since we like to <b>minimize</b> things instead of <b>maximizing</b>:
</p>


$$J = - \sum_{i=1}^{M} \sum_{j=1}^{3} y_j^{(i)} log(a_j^{(i)}) (1-y_j^{(i)}) log(1-a_j^{(i)}) $$

<p>
If we add our <b>Log Loss</b> to our computation graph:
</p>

<img class="static_image" style="width: 800px;" src="static_images/image004.png"/>


<h1>Gradient Descent</h1>

<p>
Let's see what happens if we change $w_{2, 1}$ in the network, step by step:
</p>

<img class="static_image" style="width: 900px;" src="static_images/image005.gif"/>

<p>
In order to do gradient descent, we need the derivatives:
</p>

$$
\frac{dL}{dw_{m,n}} = \sum_{i=1}^3 \left (\frac{dL}{da_i} \right) \left (\frac{da_i}{dz'_m} \right)  \left (\frac{dz'_m}{dz_m} \right ) \left(\frac{dz_m}{dw_{m,n}} \right ), \quad

\frac{dL}{db} = \sum_{i=1}^3 \left (\frac{dL}{da_i} \right) \left (\frac{da_i}{dz'_m} \right)  \left (\frac{dz'_m}{dz_m} \right ) \left(\frac{dz_m}{b_m} \right )
$$

<p>
Let's do some calculus:
<img src="static_images/evil1.png" style="width:30px; height:30px;">
<img src="static_images/evil2.png" style="width:30px; height:30px;">
</p>

$$
\frac{dL}{da_i} =  \frac{d}{da_i} \left ( - \sum_{j=1}^3 y_j log(a_j) + (1-y_j) log(1-a_j) \right ) =
\frac{d}{da_i} \left ( - y_i log(a_i) \right ) + \frac{d}{da_i} \left ( - (1-y_i) log(1-a_i) \right ) =
\frac{-y_i}{a_i} + \frac{(1-y_i)}{1-a_i}
$$

<h1>Derivative of Softmax Function</h1>

<p>
The Softmax function takes an N-dimensional vector of real values and
returns a new N-dimensional vector that sums up to $1$. The exact formula is (Softmax equation):
</p>


<p class="equation">
\begin{equation} \label{eq:softmax}
a_i = \frac{e^{z_i}}{\sum_{j=1}^N e^{z_j}}
\end{equation}
</p>

<p>
Let's make an example:
</p>

# CODE
{}
def softmax(a):
  return np.exp(a) / np.sum(np.exp(a))

a = np.array([1.0, 2.0, 3.0])
print a
print softmax(a)

# HTML
{}
<p>
Intuitively, softmax increases/emphasizes the <b>relative difference</b> between large and small values.
</p>

<p>
A nice property about the Softmax function that it produces a legit <b>Probability Distrubition</b>.
In classification (or more generally in Machine Learning) we often want to assign prababilities to
categories or classes. Softmax function is known to work well numereous applications/areas.
</p>

<p>
Softmax is a vector function -- it takes a vector as an input and returns another vector. Therefore, we
cannot just ask for <b>the derivative of softmax</b>, we can only ask the derivative of softmax regarding
particular elements.
</p>

<p>
For example,
</p>

$$
\frac{d}{d z_2} a_1
$$

<p>
refers to how much $a_1$ will change if play with $z_2$.
</p>

<p>
Using the same logic for each element for $a_i$ and $z_j$ would produce us $N \times N$ matrix of derivatives.
</p>

<p>
Let's try to take derivative for <b>one particular element</b>:
</p>

$$
\frac{d}{d z_n} a_m = \frac{d}{d z_n} \frac{e^{z_m}}{\sum_{j=1}^N e^{z_j}}
$$

<p>
We can use <b>Quotient Rule</b> here. Recall that:
</p>

$$
\frac{d}{dx} \frac{f(x)}{g(x)} = \frac{f'(x)g(x) - g'(x)f(x)}{ [g(x)]^2 }
$$

<p>
In our case:
</p>

$$
f(x) = e^{z_m}, \quad g(x) = \sum_{j=1}^N e^{z_j}
$$

<p>
Let's apply <b>Quotient Rule</b>, if $m=n$:
</p>

$$
\frac{d}{d z_n} a_m = \frac{d}{d z_n} \frac{e^{z_m}}{\sum_{j=1}^N e^{z_j}} =
\frac{ (e^{z_m})' \sum_{j=1}^N e^{z_j}  - (\sum_{j=1}^N e^{z_j})' e^{z_m} }{ [\sum_{j=1}^N e^{z_j}] ^ 2 } =
\frac{ e^{z_m} \sum_{j=1}^N e^{z_j} - e^{z_m} e^{z_n} }{ [\sum_{j=1}^N e^{z_j}] ^ 2 } =
\frac{ e^{z_m} } { \sum_{j=1}^N e^{z_j} }  \frac{ \sum_{j=1}^N - e^{z_n} } { \sum_{j=1}^N e^{z_j} } = (a_m)(1-a_n)
$$

<p>
Notice that we simplify, by plugging $a_i$ (see Equation \ref{eq:softmax}) in the last step above.
</p>

<p>
Similarly, if $m \neq n$:
</p>

$$
\frac{d}{d z_n} a_m = \frac{d}{d z_n} \frac{e^{z_m}}{\sum_{j=1}^N e^{z_j}} =
\frac{ (e^{z_m})' \sum_{j=1}^N e^{z_j}  - (\sum_{j=1}^N e^{z_j})' e^{z_m} }{ [\sum_{j=1}^N e^{z_j}] ^ 2 } =
\frac{ 0 - e^{z_m} e^{z_n} }{ [\sum_{j=1}^N e^{z_j}] ^ 2 } = -\frac{ e^{z_m} } { \sum_{j=1}^N e^{z_j} } \frac{ e^{z_n} }{ \sum_{j=1}^N e^{z_j} }
= - (a_m) (a_n)
$$

<p>
More succintly, we can summarize all above:
</p>

$$
\frac{d}{d z_n} a_m =
  \begin{cases}
    (a_m)(1-a_n), & \text{if}\ \quad  m = n \\
    - (a_m) (a_n), & \text{if}\ \quad  m \neq n
  \end{cases}
$$

<p>
We know the derivative of sigmoid function from the previous lecture:
</p>

$$
\frac{dz'}{dz} = g(z)(1-g(z))
$$

<p>
Similarly, we also calculated (and easy to see):
</p>

$$
\frac{dz}{dw_{m,n}} = x_n, \quad \frac{dz}{db_i} = 1
$$

<p>
Now, let's combine these:
</p>



# HTML
{}
<h1>References</h1>

<ul>
  <li> http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/
  <li> https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/ </li>
  <li> http://tutorial.math.lamar.edu/Classes/CalcI/ProductQuotientRule.aspx </li>
</ul>
