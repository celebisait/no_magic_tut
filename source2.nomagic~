# HTML
{}
<h1>Softmax regression for three classes</h1>

# CODE
{'type': 'image', 'width': 600}
plt.grid()

X = np.array([[-0.1, -0.5, 1.3, -0.6, -1.5, 0.2, -0.3,  0.7,  1.1, -1.0,
               -0.5, -1.3,  -1.4, -0.9, 0.4 , -0.4, 0.3, -1.6, -0.5, -1.0],
              [1.4,  -0.1, 0.9,  0.4,  0.4, 0.2, -0.4, -0.8, -1.5,  0.9,
               1.5, -0.45, -1.2, -0.7, -1.3, 0.6, -0.5, -0.7, -1.4, -1.4]])
Y = np.array([[0, 0, 1, 0, 2, 1, 1, 1, 1, 0, 0, 2, 2, 2, 1, 0, 1, 2, 2, 2]])

colormap = np.array(['r', 'b', 'g'])

def plot_scatter(X, Y, path):
   plt.scatter(X[0], X[1], s=50, c=colormap[Y[0]])
   plt.xlim([-2.0,2.0])
   plt.ylim([-2.0,2.0])
   plt.title('Input points', size=18)
   plt.savefig(path)

plot_scatter(X, Y, 'image.png')

plt.close()
plt.clf()
plt.cla()


# HTML
{}
<h1>One hot vector representation</h1>

We represent the output as a one hot vector. In other words, we represent red points using $\begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}$ and similary $\begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}$ for blues and lastly $\begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}$ for greens.

<br>
<br>
Image1
<br>
<br>

We haven't decided about a loss function for the above setting. We can use cross entropy loss function:

<br>
<br>
Image2
<br>
<br>

# HTML
{}
<h1>Appendix</h1>
<h2>Some other activation functions</h2>

<ul>
  <li>Sigmoid</li>
  <li>Tanh</li>
  <li>RELU</li>
</ul>

# CODE
{'type': 'image', 'width': 600}
plt.grid()

sigmoid = lambda x: 1/(1+np.exp(-x))
tanh    = lambda x: (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))
relu    = lambda x: np.maximum(0, x)

def plot_activation_functions():
   xs = np.arange(-4, 4, 0.001)

   plt.plot(xs, sigmoid(xs), label=r'sigmoid: $\frac{1}{1+e^{-x}}$')
   plt.plot(xs, tanh(xs), label=r'tanh: $\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$')
   plt.plot(xs, relu(xs), label=r'relu: $max(0, x)$')

   plt.legend(loc='lower right', fontsize=17)
   plt.ylim([-1.2,1.2])
   plt.savefig('image.png')

plt.title('Other activation functions', size = 18)
plot_activation_functions()

plt.close()
plt.clf()
plt.cla()

# HTML
{}
<h2> Derivatives of other activation functions </h2>

# CODE
{'type': 'image', 'width': 600}
plt.grid()

sigmoid_der = lambda x: sigmoid(x)*(1-sigmoid(x))
tanh_der    = lambda x: 1-np.tanh(x)**2
relu_der    = lambda x: (x > 0) * 1.0

def plot_activation_functions():
   xs = np.arange(-4, 4, 0.001)

   plt.plot(xs, sigmoid_der(xs), label=r"sigmoid der: $f'(x) = f(x)(1-f(x))}}$")
   plt.plot(xs, tanh_der(xs), label=r'tanh der: $1-tan^2h(x)$')
   plt.plot(xs, relu_der(xs), label=r'relu der')

   plt.legend(loc='upper left', fontsize=17)
   plt.ylim([-0.2,1.4])
   plt.savefig('image.png')

plt.title('Derivatives of activation functions', size = 18)
plot_activation_functions()

plt.close()
plt.clf()
plt.cla()

# HTML
{}
<h1>TODO</h1>

<ul>
  <li> Add proof for convexity of the loss function used in logistic regression. </li>
</ul>
