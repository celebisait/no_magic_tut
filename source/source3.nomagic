# HTML
{}
<center><h1>Part 3: Building a Simple Neural Network <span style="color: red">[Draft]</span></h1></center>
<center><b>Sait Celebi</b> (celebisait@gmail.com)</center>

# LAST_UPDATED
{}

# HTML
{}
<p>
"Truth is ever to be found in the simplicity, and not in the multiplicity and confusion of things."
-- Sir Isaac Newton
</p>

<h1>Introduction</h1>
<p>
Let's say we want to build a model to discriminate the following <b>red</b> and <b>green</b>
points in 2-dimensional space:
</p>

# CODE
{'type': 'image', 'width': 600}
import numpy as np
import matplotlib.pyplot as plt
np.set_printoptions(precision=3, suppress=True)

X = np.array([[ 0.3, 1.214, 0.052, 1.5, -0.15, -0.282, -0.464, -0.282,  -1.214, -1.5,
                -0.15, 0.052, 0.23, 1.214,-0.464, -1.214,  0.464,  0, 0.23, 0.464],
	      [ 0.0, -0.882, 0.295, 0.0, 0.26, 0.103, -1.427, -0.103, 0.882, 0.0,
	        -0.26 , -0.295, -0.193, 0.882, 1.427, -0.882, -1.427, 0, 0.193, 1.427]])
Y = np.array([[0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1]])
colormap = np.array(['r', 'g'])

def plot_scatter(X, Y, colormap, path):
   plt.grid()
   plt.xlim([-2.0, 2.0])
   plt.ylim([-2.0, 2.0])
   plt.xlabel('$x_1$', size=20)
   plt.ylabel('$x_2$', size=20)
   plt.title('Input 2D points', size=18)
   plt.scatter(X[0], X[1], s=50, c=colormap[Y[0]])
   plt.savefig(path)

plot_scatter(X, Y, colormap, 'image.png')

plt.close()
plt.clf()
plt.cla()

# HTML
{}
<p>
In other words, given a point, $(x_1, x_2)$, we want to output either <b>red</b>,
or <b>green</b>.
</p>

<p>
We can build a simple Neural Network for this problem. Neural Networks are widely used
for applications ranging from face recognition, machine translation, speech to text,
self driving cars, etc.  This is a very simple neural network in terms of number of layers and
number of neurons it contains.
</p>

<h1>Computation Graph</h1>

<p>
Here is a visual representation of our model:
</p>

<img class="static_image" style="width: 800px;" src="../static_images/image007.png"/>

<p>
and decide if $A^{(3)} > 0.5$ four our <b>final prediction</b>.
</p>

<h1>Feed-forward Phase</h1>

<p>
Let's assume that we are given the weights and biases. How do we calculate the output?
</p>

<p>
We represent $X$ as a matrix. $X$ contains all the points. In our case $X$ contains $M=20$
samples and for each sample we have $(x,y)$. $Y$ contains all the labels (red or green):
</p>

$$
X =
\begin{bmatrix}
0   &  0   & \dots &  0 \\
0   &  0   & \dots &  0 \\
\end{bmatrix}_{2 \times M}, \quad
Y =
\begin{bmatrix}
0   &  0   & \dots &  0 \\
\end{bmatrix}_{1 \times M}, \quad
$$

<p>
Here is the <b>parameters</b> of our model:
</p>

$$
W^{(1)} =
\begin{bmatrix}
0   &  0   \\
0   &  0   \\
0   &  0   \\
\end{bmatrix}_{3 \times 2}, \quad
b^{(1)} =
\begin{bmatrix}
0  \\
0  \\
0  \\
\end{bmatrix}_{3 \times 1}
$$

$$
W^{(2)} =
\begin{bmatrix}
0   &  0  &   0  \\
0   &  0  &   0  \\
0   &  0  &   0  \\
\end{bmatrix}_{3 \times 3}, \quad
b^{(2)} =
\begin{bmatrix}
0  \\
0  \\
0  \\
\end{bmatrix}_{3 \times 1}
$$

$$
W^{(3)} =
\begin{bmatrix}
0  &  0  &   0   \\
\end{bmatrix}_{1 \times 3}, \quad
b^{(3)} =
\begin{bmatrix}
0  \\
\end{bmatrix}_{1 \times 1}
$$

<p>
Feed-forward basically means given $X, Y, W^{(1)}, W^{(2)}, W^{(3)}$ and $b^{(1)}, b^{(2)}, b^{(3)}$
will produce us $A^{(3)}$. Here is step by step how we do the feed forward phase.
</p>

$$
Z^{(1)} = W^{(1)} X + b^{(1)}
$$

$$
A^{(1)} = g(Z^{(1)})
$$

$$
Z^{(2)} = W^{(2)} A^{(1)} + b^{(2)}
$$

$$
A^{(2)} = g(Z^{(2)})
$$

$$
Z^{(3)} = W^{(3)} A^{(2)} + b^{(3)}
$$

$$
A^{(3)} = g(Z^{(3)})
$$

# CODE
{}
sigmoid = lambda x: 1/(1+np.exp(-x))

def forward_propagate(X, W1, b1, W2, b2, W3, b3):
  Z1 = np.matmul(W1, X) + b1
  A1 = sigmoid(Z1)

  Z2 = np.matmul(W2, A1) + b2
  A2 = sigmoid(Z2)

  Z3 = np.matmul(W3, A2) + b3
  A3 = sigmoid(Z3)

  return Z1, A1, Z2, A2, Z3, A3

W1_initial = np.random.rand(3, 2)
W1 = W1_initial.copy()
b1 = np.zeros((3, 1))
W2_initial = np.random.rand(3, 3)
W2 = W2_initial.copy()
b2 = np.zeros((3, 1))
W3_initial = np.random.rand(1, 3)
W3 = W3_initial.copy()
b3 = np.zeros((1, 1))

Z1, A1, Z2, A2, Z3, A3 = forward_propagate(X, W1, b1, W2, b2, W3, b3)
print(Y)
print(A3)

# HTML
{}
<p>
Above we print the predictions for a random initial set of weights and bias. We are randomly
initializing weights and bias and feeding $X$ to our random initial model. As you can see,
our predictions are pretty random too, as expected. However, you can see that, given the weights,
and bias, it is pretty straight-forward to calculate the final predictions.
The tricky part is to <i>learn</i> those weights properly.
</p>

<h1> Cross Entropy Loss Function </h1>

<p>
In training, our goal is to <b>learn</b>: $W^{(1)}, W^{(2)}, W^{(3)}, b^{(1)}, b^{(2)}, b^{(3)}$ that best <b>discriminates</b>
red and green points. These are called the <b>parameters</b> of our model.
</p>

<p>
We want to find parameters that minimizes some definition of a <b>cost function</b>. We will use the same
cost function we have derived before:
</p>

$$L = - \sum_{i=1}^{M} Y_{i} log( A^{(3)}_{i} ) + (1-Y_{i}) log( 1 - A^{(3)}_{i}  )$$

<p>
Please see previous lectures if you are interested understanding how we derived the cross entropy loss function.
</p>

<p>
If we add our <b>Log Loss</b> to our computation graph:
</p>

<img class="static_image" style="width: 800px;" src="../static_images/image006.png"/>


<h1>Backpropagation</h1>

<p>
We need Backpropagation to calculate derivates so that we can use <b>Gradient Descent</b>.
In order to do gradient descent, we need to calculate the the derivate of loss w.r.t.
each parameter in our model. i.e.,
</p>

$$
\frac{dL}{dW^{(i)}}, \quad \frac{dL}{db^{(i)}}
$$

<p>
We will need to use <b>chain rule</b> to get there. Let's go step by step.
</p>

<p>
We remember that:
</p>

$$
\frac{dL}{dZ^{(3)}} = A^{(3)} - Y
$$

<p>
from the previous lectures.
</p>

<p>
Let's try to go one step further and calculate $\frac{dL}{dW^{(3)}}$ by keeping in mind that
we already know $\frac{dL}{dZ^{(3)}}$.
</p>

<p>
In other words, we know how much $L$ changes if we play
with $Z^{(3)}$ and we are trying to find out how much $L$ changes if we play with $W^{(3)}$.
</p>

<p>
Let's look at how we calculate $Z^{(3)}$:
</p>

$$

\begin{matrix}
\begin{bmatrix} 0 & 0 & 0 \end{bmatrix}_{(1 \times 3)}
\\
\\
\mbox{}
\end{matrix}

\begin{bmatrix}
0  \\
0  \\
0  \\
\end{bmatrix}_{(3 \times 1)}
=
\begin{bmatrix}
0  \\
\end{bmatrix}_{(1 \times 1)}

$$


# CODE
{'width': 600}
ALPHA = 0.3  # learning rate

# this simple implementation is numerically unstable, because:
# np.log() returns -inf for small inputs very close to 0
def get_loss(Y, Y_hat):
  loss = -1 * np.sum(Y * np.log(Y_hat) +
                     (1-Y) * np.log(1-Y_hat))
  return loss

# semantically same with above function, and numerically stable.
def get_loss_numerically_stable(Y, Z3):
  loss = -1 * np.sum(Y * -1 * np.log(1 + np.exp(-Z3)) +
                     (1-Y) * (-Z3 - np.log(1 + np.exp(-Z3))))
  return loss

def get_gradients(Z1, A1, Z2, A2, Z3, A3, Y):
  dZ3 = A3 - Y  # size (1x20)

  dW3 = (1.0/Y.size) * np.sum( (dZ3 * A2).T, axis = 0, keepdims = True )  # size (1x3)
  db3 = (1.0/Y.size) * np.sum( dZ3.T, axis = 0, keepdims = True )  # size (1x1)

  dA2 = W3.T * dZ3  # size (3x20)
  dZ2 = dA2 * (A2 * (1-A2)) # size (3x20)

  dW2 = (1.0/Y.size) * np.matmul(dZ2, A1.T)  # size (3x3)
  db2 = (1.0/Y.size) * np.sum(dZ2, axis = 1, keepdims= True)  # size (3x1)

  dA1 = np.matmul(W2.T, dZ2)  # size (3x20)
  dZ1 = dA1 * (A1 * (1 - A1))  # size (3x20)

  dW1 = (1.0/Y.size) * np.matmul(dZ1, X.T)  # size (3x2)
  db1 = (1.0/Y.size) * np.sum(dZ1, axis = 1, keepdims = True) # size (3x1)

  return dW1, db1, dW2, db2, dW3, db3

def gradient_descent(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, alpha):
  W1 = W1 - alpha * dW1
  W2 = W2 - alpha * dW2
  W3 = W3 - alpha * dW3
  b1 = b1 - alpha * db1
  b2 = b2 - alpha * db2
  b3 = b3 - alpha * db3

  return W1, W2, W3, b1, b2, b3

L_cache = []

for i in range(7000):
  Z1, A1, Z2, A2, Z3, A3 = forward_propagate(X, W1, b1, W2, b2, W3, b3)

  L = (1.0 / 20) * get_loss_numerically_stable(Y, Z3)

  dW1, db1, dW2, db2, dW3, db3 = get_gradients(Z1, A1, Z2, A2, Z3, A3, Y)

  W1, W2, W3, b1, b2, b3 = gradient_descent(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, ALPHA)

  L_cache.append(L)

plt.grid()
plt.title('Loss', size=18)
plt.xlabel('Number of iterations', size=15)
plt.ylabel('Loss', size=15)
plt.plot(L_cache)

plt.savefig('image.png')

plt.close()
plt.clf()
plt.cla()


# CODE
{}
Z1, A1, Z2, A2, Z3, A3 = forward_propagate(X, W1, b1, W2, b2, W3, b3)
print(Y)
print(A3)

# HTML
{}
<h1> Decision Boundary </h1>

<p>
Above, we simply find the boundaries and plot them. The definition of the boundary is that
the region in which the predictions are equally confident for both of the classifiers. Since
we have three classes, there are 3 choose 2 = 3 boundaries.
</p

<p>
Similarly, we can plot the same as our classifier progresses through the learning process.
As you may guess, it should start from a random point and get smarter in each step.
</p>

# HTML
{}
<p>
As you can see, it starts from a random classifier that does not seem to be working well in the
beginning. And the learning process figures out where to go next to find a better classifier.
After the learning is done, the final classifier is pretty good, in fact it has 100% accuracy.
</p>

<p>
Let's see the decision boundary in a more lazy setting. Here, we simply classify every single point
in the grid and then give the predictions to a contour plot. Comparing to the previous animation,
contour plot shows the prediction of every single point in the grid in the final version of the
classifiers parameters. On the other hand, the previous animation shows the parameters step by
step through the gradient descent iterations.
</p>

# HTML
{}
<p>
Here, the color of the background depicts our prediction for that imaginary point. Remember that our prediction
is $\mathbf{a}$ and it is three dimensional. So, we simply convert that vector to RGB space. So, for example
if the prediction is: $[0.98, 0.01, 0.01]$, it will be almost a perfect <b>red</b>, and so on.
</p>

<p>
If you look closely, you will see some <b>purple color</b> between red and blue points. That is because
the predictions in that region is something similar to $[0.45, 0.1, 0.45]$. And this means a mix of red
and blue which gives us purple. Similar phenomena happens between other decision boundary intersections.
</p>

<h1> Applying Simple Neural Network using low-level Tensorflow APIs </h1>

<p>
Here is how to train the same classifier for the above red, green and blue points using low-level TensorFlow API.
It produces <b>almost exact</b> output with our own hand crafted model. Be aware that there may be small differences
because of the initial random start of both models. (Remember that $W$ is initialized with random values.)
</p>

# CODE
{'width': 600}
import tensorflow as tf

t_X = tf.placeholder(tf.float32, [2, None])
t_Y = tf.placeholder(tf.float32, [1, None])

# t_W1 = tf.Variable(tf.random_uniform((3, 2)))
t_W1 = tf.Variable(W1_initial.astype('f'))
t_b1 = tf.Variable(tf.zeros([3, 1]))

# t_W2 = tf.Variable(tf.random_uniform((3, 3)))
t_W2 = tf.Variable(W2_initial.astype('f'))
t_b2 = tf.Variable(tf.zeros([3, 1]))

#t_W3 = tf.Variable(tf.random_uniform((1, 3)))
t_W3 = tf.Variable(W3_initial.astype('f'))
t_b3 = tf.Variable(tf.zeros([1]))

t_Z1 = tf.matmul(t_W1, t_X) + t_b1
t_A1 = tf.sigmoid(t_Z1)

t_Z2 = tf.matmul(t_W2, t_A1) + t_b2
t_A2 = tf.sigmoid(t_Z2)

t_Z3 = tf.matmul(t_W3, t_A2) + t_b3
t_A3 = tf.sigmoid(t_Z3)

t_Loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = t_Z3,  labels = t_Y))

train = tf.train.GradientDescentOptimizer(0.3).minimize(t_Loss)
init = tf.global_variables_initializer()

with tf.Session() as session:
   session.run(init)
   losses = []
   for i in range(7000):
      ttrain, ttloss = session.run([train, t_Loss], feed_dict={t_X:X, t_Y:Y})
      losses.append(ttloss)

   print(session.run([t_A3], feed_dict={t_X:X, t_Y:Y})[0])
   print(Y)


plt.grid()
plt.plot(losses)
plt.title('Tensorflow Loss', size = 18)
plt.xlabel('Number of iterations', size=15)
plt.ylabel('Loss', size=15)
plt.savefig('image.png')

plt.close()
plt.clf()
plt.cla()

# HTML
{}

<h1>Exercises</h1>
<ol>
  <li>
   1
  </li>
  <li>
   2
  </li>
</ol>

<h1>References</h1>

<ul>
  <li> http://karpathy.github.io/ </li>
  <li> http://colah.github.io/ </li>
  <li> https://github.com/tensorflow/workshops </li>
</ul>
