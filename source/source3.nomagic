# HTML
{}
<center><h1>Part 3: Building a Simple Neural Network <span style="color: red">[Draft]</span></h1></center>
<center><b>Sait Celebi</b> (celebisait@gmail.com)</center>

# LAST_UPDATED
{}

# HTML
{}
<p>
"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua."
-- Cicero
</p>

<h1>Introduction</h1>
<p>
Let's say we want to build a model to discriminate the following <b>red</b> and <b>green</b>
points in 2-dimensional space:
</p>

# CODE
{'type': 'image', 'width': 600}
import numpy as np
import matplotlib.pyplot as plt
np.set_printoptions(precision=3, suppress=True)


X = np.array([[ 0.3, 0.23, 0.052, -0.15, -0.282, -0.282, -0.15, 0.052,
                0.23, 1.5, 1.214, 0.464, -0.464, -1.214, -1.5, -1.214,
	        -0.464,  0.464,  1.214, 0],
	      [ 0.0, 0.193, 0.295, 0.26, 0.103, -0.103, -0.26 , -0.295,
	        -0.193, 0.0, 0.882, 1.427, 1.427, 0.882, 0.0, -0.882,
		-1.427, -1.427, -0.882, 0]])
Y = np.array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])
colormap = np.array(['r', 'g'])

def plot_scatter(X, Y, colormap, path):
   plt.grid()
   plt.xlim([-2.0, 2.0])
   plt.ylim([-2.0, 2.0])
   plt.xlabel('$x_1$', size=20)
   plt.ylabel('$x_2$', size=20)
   plt.title('Input 2D points', size=18)
   plt.scatter(X[0], X[1], s=50, c=colormap[Y[0]])
   plt.savefig(path)

plot_scatter(X, Y, colormap, 'image.png')

plt.close()
plt.clf()
plt.cla()

# HTML
{}
<p>
In other words, given a point, $(x_1, x_2)$, we want to output either <b>red</b>,
or <b>green</b>.
</p>

<p>
We can build a simple Neural Network for this problem. Neural Networks are widely used
for applications ranging from face recognition, machine translation, speech to text,
self driving cars, etc.  This is probably the simplest neural networks you will see.
</p>

<h1>Computation Graph</h1>

<p>
Here is a visual representation of our model:
</p>

<img class="static_image" style="width: 800px;" src="../static_images/image003.png"/>

<p>
and simply pick the biggest $a_i$ to do the <b>final prediction</b>.
</p>

<h1>Feed-forward Phase</h1>

<p>
Let's assume that we are given the weights and bias. How do we calculate the output?
</p>

<p>
We represent $X$ as a matrix. $X$ contains all the points. In our case $X$ contains $M=20$
samples and for each sample we have $(x,y)$. $Y$ contains all the labels (red, green and blue)
as a one hot vector. $W$ has the weights. $b$ has the bias:
</p>

$$
X =
\begin{bmatrix}
0   &  0   & \dots &  0 \\
0   &  0   & \dots &  0 \\
\end{bmatrix}_{2 \times M}, \quad
Y =
\begin{bmatrix}
0   &  0   & \dots &  0 \\
0   &  0   & \dots &  0 \\
0   &  0   & \dots &  0 \\
\end{bmatrix}_{3 \times M}, \quad
W =
\begin{bmatrix}
0   &  0   \\
0   &  0   \\
0   &  0   \\
\end{bmatrix}_{3 \times 2}
b =
\begin{bmatrix}
0  \\
0  \\
0  \\
\end{bmatrix}_{3 \times 1}
$$

<p>
Feed-forward basically means given $X, Y, W$ and $b$, will produce us $a$ and $L$.
</p>

$$
Z = W X + b
$$

<p>
Here we can see it visually:
</p>

$$
\begin{bmatrix}
0   &  0   & \dots &  0 \\
0   &  0   & \dots &  0 \\
0   &  0   & \dots &  0 \\
\end{bmatrix}_{3 \times M}
=
\begin{bmatrix}
0   &  0   \\
0   &  0   \\
0   &  0   \\
\end{bmatrix}_{3 \times 2}
\begin{bmatrix}
0   &  0   & \dots &  0 \\
0   &  0   & \dots &  0 \\
\end{bmatrix}_{2 \times M}
+
\begin{bmatrix}
0  \\
0  \\
0  \\
\end{bmatrix}_{3 \times 1}
$$

# CODE
{}
sigmoid = lambda x: 1/(1+np.exp(-x))

def forward_propagate(X, W1, b1, W2, b2, W3, b3):
  Z1 = np.matmul(W1, X) + b1
  A1 = sigmoid(Z1)

  Z2 = np.matmul(W2, A1) + b2
  A2 = sigmoid(Z2)

  Z3 = np.matmul(W3, A2) + b3
  A3 = sigmoid(Z3)

  return Z1, A1, Z2, A2, Z3, A3

W1 = np.random.rand(3, 2)
b1 = np.zeros((3, 1))
W2 = np.random.rand(3, 3)
b2 = np.zeros((3, 1))
W3 = np.random.rand(1, 3)
b3 = np.zeros((1, 1))

Z1, A1, Z2, A2, Z3, A3 = forward_propagate(X, W1, b1, W2, b2, W3, b3)
print(Y[0:10])
print(A3[:,0:10])

# HTML
{}
<p>
This is backprop.
</p>

# CODE
{}
ALPHA = 2.0 # learning rate

# this simple implementation is numerically unstable, because:
# np.log() returns -inf for small inputs very close to 0
def get_loss(Y, Y_hat):
  loss = -1 * np.sum(Y * np.log(Y_hat) +
                      (1-Y) * np.log(1-Y_hat))
  return loss

# semantically same with above function, and numerically stable.
def get_loss_numerically_stable(Y, A3):
  loss = -1 * np.sum(Y * -1 * np.log(1 + np.exp(-A3)) +
                      (1-Y) * (-A3 - np.log(1 + np.exp(-A3))))
  return loss

def get_gradients(Z1, A1, Z2, A2, Z3, A3):
  dA = (-Y_one_hot / A)

  dZ = np.zeros((3, 20))
  for i in range(20):
    a = A[:, [i]]
    da = dA[:, [i]]
    matrix = np.matmul(a, np.ones((1, 3))) * (np.identity(3) - np.matmul(np.ones((3, 1)), a.T))
    dZ[:, [i]] = np.matmul(matrix, da)

  dW = np.zeros((3,2))
  db = np.zeros((3,1))
  for i in range(20):
    x = X[:, [i]]
    dz = dZ[:, [i]]
    dw = dz * np.tile(x, 3).T
    dW += (1.0/20) * dw
    db += (1.0/20) * dz

  return dZ, dW, db

def gradient_descent(W, b, dW, db, alpha):
  W = W - alpha * dW
  b = b - alpha * db
  return W, b

# random initialization
W = np.random.rand(3, 2)
b = np.zeros((3, 1))

W_cache = []
b_cache = []
L_cache = []

for i in range(1):
  Z1, A1, Z2, A2, Z3, A3 = forward_propagate(X, W1, b1, W2, b2, W3, b3)
  # L = (1.0 / 20) * get_loss(Y, A3)
  L = (1.0 / 20) * get_loss_numerically_stable(Y, A3)

  dZ3 = A3 - Y
  dW3 = dZ3 * A2
  db3 = dZ3

  dA2 = dZ3 * (A2 * (1-A2))

  print(dZ3.shape)
  print(dZ3)
  print(dW3.shape)
  print(dW3)

# HTML
{}
<p>
As you may realized, the summation here is called <b>broadcasting</b>.
</p>

<p>
After getting $Z$, we apply softmax function over $Z$:
</p>

<p class="equation">
\begin{equation} \label{eq:softmax}
a_i = \frac{e^{z_i}}{\sum_{j=1}^N e^{z_j}}
\end{equation}
</p>

<p>
However, this may be problematic to compute for big values of $z_i$. We call this phenomena
<b>numerically unstable</b>. Because $e^{z_i}$ easily overflows 64bit (even 128bit).
We need to approach it slightly differently.
</p>

# HTML
{}
<p>
Above we print the first 10 predictions, and they look pretty good. In fact, we have 100% accuracy.
So, given the weights, and bias, it is pretty straight-forward to calculate the final predictions.
The tricky part is to <i>learn</i> those weights properly.
</p>

# HTML
{}
<h1> Cross Entropy Loss Function </h1>

<p>
In training, our goal is to <b>learn</b> a matrix $W$ of size $(3 \times 2)$  and a $\mathbf{b}$ of size $(3 \times 1)$
that best <b>discriminates</b> red, green and blue points.
</p>

<p>
We want to find $W$ and $\mathbf{b}$ that minimizes some definition of a <b>cost function</b>.
Let's attempt to write a cost function for this problem.
</p>

<p>
Let's say we have three points:
</p>

$$\mathbf{x} = \begin{bmatrix} -0.1 \\ 1.4 \end{bmatrix}, y=0$$

$$\mathbf{x} = \begin{bmatrix} 1.3  \\ 0.9 \end{bmatrix}, y=1$$

<p>
and similarly,
</p>

$$\mathbf{x} = \begin{bmatrix} -1.4  \\ -1.1 \end{bmatrix}, y=2$$

<p>
Now, let's list these $y$ as a <b>one hot vector</b> and, their corresponding <i>imaginary</i> $\mathbf{a}$ values:
</p>

$$\mathbf{y} = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, \mathbf{a} = \begin{bmatrix} 0.9 \\ 0.1 \\ 0.0 \end{bmatrix} $$

$$\mathbf{y} = \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}, \mathbf{a} = \begin{bmatrix} 0.1 \\ 0.8 \\ 0.1 \end{bmatrix} $$

$$\mathbf{y} = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}, \mathbf{a} = \begin{bmatrix} 0.1 \\ 0.2 \\ 0.7 \end{bmatrix} $$

<p>
Intuitively, we want a classifier that produces <b>similar</b> looking $\mathbf{a}$ and $\mathbf{y}$.
This means, if
$\mathbf{y} = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}$, then, for example, having
$\mathbf{a} = \begin{bmatrix} 0.8 \\ 0.1 \\ 0.1 \end{bmatrix}$ is <b>more desirable</b> than having
$\mathbf{a} = \begin{bmatrix} 0.6 \\ 0.2 \\ 0.2 \end{bmatrix}$.

<p>
In other words, we want to <b>maximize</b>:
</p>

$$P(\mathbf{y}|\mathbf{x}) = \prod_{j=1}^{3} a_j^{y_j} $$

<p>
Here, $a_j$ represents the jth item in the vector $\mathbf{a}$, and similarly $y_j$ represents the jth value in $\mathbf{y}$.
For example, when $\mathbf{a} = \begin{bmatrix} 0.9 \\ 0.1 \\ 0.0 \end{bmatrix}$, then, $a_1 = 0.9, a_2 = 0.1$ and
$a_3 = 0.0$.
</p>

$$\mathbf{y} = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, \mathbf{a} = \begin{bmatrix} 0.9 \\ 0.1 \\ 0.0 \end{bmatrix},
P(\mathbf{y}|\mathbf{x}) = 0.9 \times 1 \times 1 = 0.9 $$

$$\mathbf{y} = \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}, \mathbf{a} = \begin{bmatrix} 0.1 \\ 0.8 \\ 0.1 \end{bmatrix},
P(\mathbf{y}|\mathbf{x}) = 1 \times 0.8 \times 1 = 0.8 $$

$$\mathbf{y} = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}, \mathbf{a} = \begin{bmatrix} 0.1 \\ 0.2 \\ 0.7 \end{bmatrix},
P(\mathbf{y}|\mathbf{x}) = 1 \times 1 \times 0.7 = 0.7 $$

<p>
<b>Bigger</b> the $P(\mathbf{y}|\mathbf{x})$ is the <b>better</b>.
</p>

<p>
Similar to <b>Logistic Regression</b>, in order to define the <b>loss</b> for multiple samples, we will simply multiply
each value for each sample (<b>Maximum Likelihood Estimation</b>):
</p>

$$J = \prod_{i=1}^{M} P(y^{(i)}|x^{(i)}) $$

<p>
here, $y^{(i)}$, $x^{(i)}$ and $a^{(i)}$ corresponds to ith sample in the training set out of $M$ training samples. We can rewrite it as:
</p>

$$J = \prod_{i=1}^{M} \prod_{j=1}^{3}  (a_j^{(i)}) ^ {y_j^{(i)}} $$

<p>
Maximizing above is equal to maximizing:
</p>

$$J = log \left( \prod_{i=1}^{M} \prod_{j=1}^{3} (a_j^{(i)}) ^ {y_j^{(i)}} \right ) $$

<p>
we can write it as:
</p>

$$J = \sum_{i=1}^{M} \sum_{j=1}^{3} y_j^{(i)} log(a_j^{(i)}) $$

<p>
since we like to <b>minimize</b> things instead of <b>maximizing</b>:
</p>

$$J = - \sum_{i=1}^{M} \sum_{j=1}^{3} y_j^{(i)} log(a_j^{(i)}) $$

<p>
If we add our <b>Log Loss</b> to our computation graph:
</p>

<img class="static_image" style="width: 900px;" src="../static_images/image004.png"/>

<p>
Also, this loss function is sometimes called <b>Cross Entropy Loss Function</b> in some contexts.
</p>

<h1>Gradient Descent</h1>

<p>
Ideally, we want to start with <b>random</b> parameters and make our parameters
better and better gradually as an iterative manner. Gradient descent is simply:
</p>

$$
W = W - \alpha \frac{dL}{dW} \quad \mathbf{b} = \mathbf{b} - \alpha \frac{dL}{d \mathbf{b}}
$$

<p>
The tricky part here is to compute $\frac{dL}{dW}$ and $\frac{dL}{d \mathbf{b}}$. We need
to do a small scale <b>back propagation</b> of derivatives here.
</p>

<p>
But first, let's see what happens if we change $w_{2, 1}$ in the network, step by step:
</p>

<img class="static_image" style="width: 900px;" src="../static_images/animation001.gif"/>

<p>
In order to do gradient descent, we need the derivatives:
</p>

$$
\frac{dL}{dw_{m,n}} = \sum_{i=1}^3 \left (\frac{dL}{da_i} \right) \left (\frac{da_i}{dz_m} \right) \left(\frac{dz_m}{dw_{m,n}} \right ), \quad

\frac{dL}{db_m} = \sum_{i=1}^3 \left (\frac{dL}{da_i} \right) \left (\frac{da_i}{dz_m} \right) \left(\frac{dz_m}{b_m} \right )
$$

<p>
Let's do some calculus:
<img src="../static_images/evil1.png" style="width:30px; height:30px;">
<img src="../static_images/evil2.png" style="width:30px; height:30px;">
</p>

$$
\frac{dL}{da_i} =  \frac{d}{da_i} \left ( - \sum_{j=1}^3 y_j log(a_j) \right ) =
\frac{d}{da_i} \left ( - y_i log(a_i) \right ) = \frac{-y_i}{a_i}
$$

$$
\frac{dz_m}{dw_{m,n}} = x_n, \quad
\frac{dz_m}{b_m} = 1
$$

<h1>Derivative of Softmax Function</h1>

$$
\frac{d}{d z_m} a_i =
\begin{cases}
(a_i)(1-a_m),  & \text{if}\ \quad  i = m \\
- (a_m) (a_i), & \text{if}\ \quad  i \neq m
\end{cases}
$$

<p>
or equally:
</p>

$$
\frac{d}{d z_m} a_i = (a_i)(\delta_{i,m} - a_m)
$$

<p>
where $\delta_{i,m} = 1$ if $i=m$, and $0$ otherwise.
</p>

<p>
The reason we want to write it this way is that we don't want to use any <b>loops</b>. And we can
execute the above using matrix operations like:
</p>

$$
\frac{d}{dz} a = \mathbf{a} \mathbf{e^T} \circ (\mathbf{I} - \mathbf{e} \mathbf{a^T})
$$

<p>
where $\mathbf{e}$ is a vector of $1$'s of size $K\times1$ for a suitable $K$ and $\circ$ represents Hadamard product,
in other words element-wise product of matrices. And it is of size $(3 \times 3)$. We plug this below:
</p>

$$
\begin{bmatrix} \frac{da_1}{dz_1} \frac{da_2}{dz_1} \frac{da_3}{dz_1} \\
	\frac{da_1}{dz_2} \frac{da_2}{dz_2} \frac{da_3}{dz_2} \\
	\frac{da_1}{dz_3} \frac{da_2}{dz_3} \frac{da_3}{dz_3}
\end{bmatrix}_{3 \times 3}
\begin{bmatrix} \frac{dL}{da_1} \\
	\frac{dL}{da_2} \\
	\frac{dL}{da_3}
\end{bmatrix}_{3 \times 1}
=
\begin{bmatrix} \frac{dL}{dz_1} \\
	\frac{dL}{dz_2} \\
	\frac{dL}{dz_3}
\end{bmatrix}_{3 \times 1}
$$

<p>
This is exactly what we are going to do. However, this is defined for only one sample. We don't want
to loop over each sample and compute this sequantially. Ideally, we want to do everything using
matrix operations including this step. However, it is a bit trickier. So, first let's see if this
approach works using loops. After, we will see how to convert this to fully matrix operations.
</p>

<h1> Back propagation Phase </h1>

<p>
We will <i>learn</i> the weights using <b>Back Propagation</b>.
</p>

# HTML
{}
<h1> Decision Boundary </h1>

<p>
So, essentially, we have 3 equations:
</p>

$$
\text{Equation A.} \quad w_{1,1} x_1 + w_{1,2} x_2 + b_1 = z_1 \\
\text{Equation B.} \quad w_{2,1} x_1 + w_{2,2} x_2 + b_2 = z_2 \\
\text{Equation C.} \quad w_{3,1} x_1 + w_{3,2} x_2 + b_3 = z_3
$$

<p>
We have 3 boundaries between all 3 choose 2 of above:

<ul>
<li> Equation A - Equation B </li>
<li> Equation A - Equation C </li>
<li> Equation B - Equation C </li>
</ul>

If we focus on Equation A - Equation B:
</p>

$$
w_{1,1} x_1 + w_{1,2} x_2 + b_1 = w_{2,1} x_1 + w_{2,2} x_2 + b_2 \\
w_{1,1} x_1 - w_{2,1} x_1 + w_{1,2} x_2 - w_{2,2} x_2 = b_2 - b_1 \\
(w_{1,1} - w_{2,1}) x_1 + (w_{1,2} - w_{2,2}) x_2 = b_2 - b_1 \\
$$

<p>
If we plot these three lines:
</p>

# HTML
{}
<p>
Above, we simply find the boundaries and plot them. The definition of the boundary is that
the region in which the predictions are equally confident for both of the classifiers. Since
we have three classes, there are 3 choose 2 = 3 boundaries.
</p

<p>
Similarly, we can plot the same as our classifier progresses through the learning process.
As you may guess, it should start from a random point and get smarter in each step.
</p>

# HTML
{}
<p>
As you can see, it starts from a random classifier that does not seem to be working well in the
beginning. And the learning process figures out where to go next to find a better classifier.
After the learning is done, the final classifier is pretty good, in fact it has 100% accuracy.
</p>

<p>
Let's see the decision boundary in a more lazy setting. Here, we simply classify every single point
in the grid and then give the predictions to a contour plot. Comparing to the previous animation,
contour plot shows the prediction of every single point in the grid in the final version of the
classifiers parameters. On the other hand, the previous animation shows the parameters step by
step through the gradient descent iterations.
</p>

# HTML
{}
<p>
Here, the color of the background depicts our prediction for that imaginary point. Remember that our prediction
is $\mathbf{a}$ and it is three dimensional. So, we simply convert that vector to RGB space. So, for example
if the prediction is: $[0.98, 0.01, 0.01]$, it will be almost a perfect <b>red</b>, and so on.
</p>

<p>
If you look closely, you will see some <b>purple color</b> between red and blue points. That is because
the predictions in that region is something similar to $[0.45, 0.1, 0.45]$. And this means a mix of red
and blue which gives us purple. Similar phenomena happens between other decision boundary intersections.
</p>

<h1> Gradient Descent Parameter Updates </h1>

<p> Here we see the updates of parameters step by step in the gradient descent. </p>

# HTML
{}
<p>
This is a nice animation that shows the progress of our parameters $W$ and $\mathbf{b}$
in each iteration of gradient descent along with the corresponding loss value using the
given parameters.
</p>

<p>
We are starting with small random initial values for: $w_{1,1}, w_{1,2}, w_{2,1}, w_{2,2},
w_{3,1}, w_{3,2}$. We start with all $0$ values for $b_1, b_2, b_3$. Then, we start applying
gradient descent. Every step we take in the gradient descent is giving us a better set of parameters
so that we see that the loss is decreasing.
</p>

<h1> Applying Softmax Regression using low-level Tensorflow APIs </h1>

<p>
Here is how to train the same classifier for the above red, green and blue points using low-level TensorFlow API.
It produces <b>almost exact</b> output with our own hand crafted model. Be aware that there may be small differences
because of the initial random start of both models. (Remember that $W$ is initialized with random values.)
</p>


# HTML
{}

<h1>Exercises</h1>
<ol>
  <li>
   1
  </li>
  <li>
   2
  </li>
</ol>

<h1>References</h1>

<ul>
  <li> http://karpathy.github.io/ </li>
  <li> http://colah.github.io/ </li>
</ul>
