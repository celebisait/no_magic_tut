# HTML
{}
<center><h1>Part 3: Building a Simple Neural Network <span style="color: red">[Draft]</span></h1></center>
<center><b>Sait Celebi</b> (celebisait@gmail.com)</center>

# LAST_UPDATED
{}

# HTML
{}
<p>
"Truth is ever to be found in the simplicity, and not in the multiplicity and confusion of things."
-- Sir Isaac Newton
</p>

# HEADER
{'header': 'Introduction'}

# HTML
{}

<p>
Let's say we want to build a model to discriminate the following <b>red</b> and <b>green</b>
points in 2-dimensional space:
</p>

# CODE
{'type': 'image', 'width': 600}
import numpy as np
import matplotlib.pyplot as plt
import collections
plt.style.use('classic')
np.set_printoptions(precision=3, suppress=True)

X = np.array([[ 0.65, 0.00],
              [ 0.00,-0.65],
	      [-0.75,-1.29],
	      [ 1.29, 0.75],
	      [-0.46,-0.46],
	      [-1.29,-0.75],
	      [ 0.00, -1.5],
	      [ 0.46, 0.46],
	      [ 0.46,-0.46],
	      [-1.29, 0.75],
 	      [-0.46, 0.46],
	      [ 0.00, 0.65],
	      [ 0.75,-1.29],
	      [ 1.29,-0.75],
	      [-0.65, 0.00],
	      [ 1.50, 0.00],
	      [-1.50, 0.00],
	      [ 0.75, 1.29],
	      [-0.75, 1.29],
	      [ 0.00, 1.50]])
y = np.array([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1])

colormap = np.array(['r', 'g'])

def plot_scatter(X, Y, colormap, path):
   plt.grid()
   plt.xlim([-2.0, 2.0])
   plt.ylim([-2.0, 2.0])
   plt.xlabel('$x_1$', size=20)
   plt.ylabel('$x_2$', size=20)
   plt.title('Input 2D points', size=18)
   plt.scatter(X[:,0], X[:,1], s=50, c=colormap[y])
   plt.savefig(path)

plot_scatter(X, y, colormap, 'image.png')

plt.close()
plt.clf()
plt.cla()

# HTML
{}
<p>
In other words, given a point, $(x_1, x_2)$, we want to output either <b>red</b>,
or <b>green</b>. (In this tutorial 0.0 means red and 1.0 means green.)
</p>

<p>
We can build a simple Neural Network for this problem. Neural Networks are widely used
for applications ranging from face recognition, machine translation, speech to text,
self driving cars, etc.  This is a very simple neural network in terms of number of layers and
number of neurons it contains.
</p>

<p>
It's basically a Logistic Regression (or Softmax Regression) with <b>more layers</b>.
</p>

<h1>Computation Graph</h1>

<p>
Here is the visual representation of our model:
</p>

<img class="static_image" style="width: 800px;" src="../static_images/image006.png"/>

<p>
and decide if $a^{[3]} > 0.5$ for our <b>final prediction</b>.
</p>

<h1>Feed-forward Phase</h1>

<p>
Let's assume that we are given the weights and biases. How do we calculate the output?
</p>

<p>
We represent $\mathbf{X}$ as a matrix. $\mathbf{X}$ contains all the points.
In our case $\mathbf{X}$ contains $m=20$ samples. $\mathbf{y}$ contains all the
labels (red or green):
</p>

<p>
Here is the sizes of the matrices:
</p>

<ul>
  <li> $\mathbf{X}: 20 \times 2$ </li>
  <li> $\mathbf{y}: 20 \times 1$ </li>
  <li> $\mathbf{W}^{[1]}: 3 \times 2$ </li>
  <li> $\mathbf{b}^{[1]}: 3 \times 1$ </li>
  <li> $\mathbf{W}^{[2]}: 3 \times 3$ </li>
  <li> $\mathbf{b}^{[2]}: 3 \times 1$ </li>
  <li> $\mathbf{W}^{[3]}: 1 \times 3$ </li>
  <li> $b^{[3]}: 1 \times 1$ </li>
</ul>

<p>
Feed-forward basically means given:
$\mathbf{x}, y, \mathbf{W}^{[1]}, \mathbf{W}^{[2]}, \mathbf{W}^{[3]}$
and $\mathbf{b}^{[1]}, \mathbf{b}^{[2]}, b^{[3]}$
will produce us $a^{[3]}$. Here is step by step how we do the feed forward phase.
</p>

$$
\mathbf{z}^{[1]} = \mathbf{W}^{[1]} \mathbf{x} + \mathbf{b}^{[1]}
$$

$$
\mathbf{a}^{[1]} = g(\mathbf{z}^{[1]})
$$

$$
\mathbf{z}^{[2]} = \mathbf{W}^{[2]} \mathbf{a}^{[1]} + \mathbf{b}^{[2]}
$$

$$
\mathbf{a}^{[2]} = g(\mathbf{z}^{[2]})
$$

$$
z^{[3]} = \mathbf{W}^{[3]} \mathbf{a}^{[2]} + b^{[3]}
$$

$$
a^{[3]} = g(z^{[3]})
$$

<p>
We are using sigmoid function as the activation $g()$ function as we used before.
</p>

<p>
Let's try to apply the above equations using an <b>initial</b> random set of weights and bias.
</p>

# CODE
{}
sigmoid = lambda x: 1/(1+np.exp(-x))

def forward_propagate(x, W1, b1, W2, b2, W3, b3):
  z1 = np.matmul(W1, x) + b1
  a1 = sigmoid(z1)

  z2 = np.matmul(W2, a1) + b2
  a2 = sigmoid(z2)

  z3 = np.matmul(W3, a2) + b3
  a3 = sigmoid(z3)

  return z1, a1, z2, a2, z3, a3

W1_initial = np.random.rand(3, 2)
W1 = W1_initial.copy()
b1 = np.zeros((3, 1))
W2_initial = np.random.rand(3, 3)
W2 = W2_initial.copy()
b2 = np.zeros((3, 1))
W3_initial = np.random.rand(1, 3)
W3 = W3_initial.copy()
b3 = np.zeros((1, 1))

z1, a1, z2, a2, z3, a3 = forward_propagate(X[0, :].reshape(2,1), W1, b1, W2, b2, W3, b3)
print(y[0])
print(a3)

# HTML
{}
<p>
Above we print our prediction for the first point out of 20 points.
We use random initial set of weights and bias. We are <b>randomly
initializing</b> weights and bias and feed the first $\mathbf{x}$
to our random initial model. As you can see, our prediction is pretty random
too, as expected. However, you can see that, given the weights,
and bias, it is pretty straight-forward to calculate the final prediction.
The tricky part is to <i>learn</i> those weights properly.
</p>

# HEADER
{'header': 'Cross Entropy Loss Function'}

# HTML
{}
<p>
In training, our goal is to <b>learn</b>: $\mathbf{W}^{[1]}, \mathbf{W}^{[2]},
\mathbf{W}^{[3]}, \mathbf{b}^{[1]}, \mathbf{b}^{[2]}, \mathbf{b}^{[3]}$
that best <b>discriminates</b> red and green points. These are called the
<b>parameters</b> of our model.
</p>

<p>
We want to find parameters that minimizes some definition of a <b>cost function</b>. We will use the same
cost function we have used before in Logistic Regression and Softmax Regression:
</p>

$$ \text{L} = - \sum_{i=1}^{M} y^{(i)} \text{log}( a^{[3](i)} ) + (1 - y^{(i)}) \text{log}( 1 - a^{[3](i)} )$$

<p>
where $a^{[3](i)}$ is the last layer's activation value corresponding to the $i$ th item in the dataset.
</p>

<p>
Please see previous lectures if you are interested understanding how we derived the cross entropy loss function.
</p>

<p>
If we add our <b>loss</b> to our computation graph:
</p>

<img class="static_image" style="width: 800px;" src="../static_images/image007.png"/>

# HEADER
{'header': 'Backpropagation'}

# HTML
{} 
<p>
Now, we are going to have some <b>fun</b>.
</p>

<p>
Backprop is probably one of the most <b>fundamental</b> algorithms/techniques in Deep Learning.
In my opinion, mastering backprop is curicial to be able to
(1) fully understand deep learing techniques and (2) to be able to contribute new approaches in the field.
</p>

<p>
Backpropagation is simply calculating derivatives of loss w.r.t. each parameter in our model.
We need this information because we need to feed the derivatives to <b>Gradient Descent</b>.
The parameters of the model we want to compute with backprop is:
</p>


$$
\frac{d \text{L}}{d \mathbf{W}^{[i]}}, \quad \frac{d \text{L}}{d \mathbf{b}^{[i]}} \quad \forall i \in \{1,2,3\}
$$

<p>
We need to use <b>chain rule</b>, <b>iteratively</b>, to get there. The direction we apply chain rule will be
the opposite direction to the feed forward direction. In fact the name backpropagation
comes from the fact that we propagate the error (well actually the derivatives, really)
through backwards (compared to the feed-forward direction) in a step by step manner.
</p>

<p>
Here is our updated computation graph after we add the Backprop details.
</p>

<img class="static_image" style="width: 800px;" src="../static_images/image008.png"/>

<p>
Recall that we already calculated the first step of derivatives:
</p>

$$
\frac{d \text{L}}{dz^{[3]}} = a^{[3]} - y
$$

<p>
in Logistic Regression tutorial. Notice that we are using <b>exactly</b> same loss function.
(Both Logistic Regression and the model discussed in this tutorial are a binary classification task.)
</p>

<p>
Notice that this is defined <b>per sample</b>. So, we have a different $\frac{d \text{L}}{d z^{[3]}}$
value for each sample.
</p>

<p>
Let's try to go one step further and calculate $\frac{d \text{L}}{d \mathbf{W}^{[3]}}$ by keeping
in mind that we already know $\frac{d \text{L}}{dz^{[3]}}$. (This sentence mimicks why we need
<b>Chain Rule</b>)
</p>

<p>
In other words, we know how much $\text{L}$ changes if we play
with $z^{[3]}$ and we are trying to find out how much $\text{L}$ changes if we play with
$\mathbf{W}^{[3]}$.
</p>

<p>
Let's look at how we calculate $z^{[3]}$:
</p>

$$
\begin{matrix}
(\mathbf{W}^{[3]}) \\
\begin{bmatrix} 0 & 0 & 0 \end{bmatrix}
\\
\\
\mbox{}
\end{matrix}

\begin{matrix}
(\mathbf{a}^{[2]}) \\
\begin{bmatrix}
0  \\
0  \\
0  \\
\end{bmatrix}
\end{matrix}

+

\begin{matrix}
(b^{[3]})
\\
\begin{bmatrix}
0
\end{bmatrix}
\\
\\
\end{matrix}

=

\begin{matrix}
(z^{[3]})
\\
\begin{bmatrix}
0
\end{bmatrix}
\\
\\
\end{matrix}
$$

<p>
Let's look at this equation very carefully. Our simpler task for now is: "How much
$z^{[3]}$ will be affected if we play with $\mathbf{W}^{[3]}$?", i.e.,
</p>

$$
\frac{d z^{[3]}}{d \mathbf{W}^{[3]}}
$$

<p>
After finding out what the above is, we will extend it and find:
$\frac{d \text{L}}{d \mathbf{W}^{[3]}}$ using the Chain Rule.
</p>

<p>
It seems obvious <b>visually</b> that if we change, say,
$\mathbf{W}^{[3]}_1$, $z^{[3]}$ will change proportional to $\mathbf{a}^{[2]}_1$.
Notice that we are assuming that $\mathbf{a}^{[2]}$ and $b^{[3]}$ is fixed and frozen.
Using similar logic for the other items of $\mathbf{W}^{[3]}$, we can conclude:
</p>

$$
\frac{d z^{[3]}}{d \mathbf{W}^{[3]}} = \mathbf{a}^{[2]}.\text{T}
$$

<p>
Similarly, if we change $b^{[3]}$ a small amount, $z^{[3]}$ will also change as same amount
in the same direction, so:
</p>

$$
\frac{d z^{[3]}}{d b^{[3]}} = 1
$$

<p>
To finalize the calculation of $\frac{d \text{L}}{d \mathbf{W}^{[3]}}$, <b>for one sample</b>:
</p>

$$
\frac{d \text{L}}{d \mathbf{W}^{[3]}} = \frac{d \text{L}}{d z^{[3]}} \frac{d z^{[3]}}{d \mathbf{W}^{[3]}}
$$

<p>
and similarly for $\frac{d \text{L}}{d b^{[3]}}$:
</p>

$$
\frac{d \text{L}}{d b^{[3]}} = \frac{d \text{L}}{d z^{[3]}}
$$

<p>
Notice that $\frac{d \text{L}}{dz^{[3]}} \in \mathbb{R}$ and $\frac{dz^{[3]}}{d \mathbf{W}^{[3]}} \in \mathbb{R}^3$.
So, it is a scalar multiplication.
</p>

<p>
Nice.
</p>

<p>
At this moment, we calculated $\frac{d \text{L}}{d \mathbf{W}^{[3]}}$ and
$\frac{d \text{L}}{d b^{[3]}}$. Now, we want to calculate
$\frac{d \text{L}}{d \mathbf{W}^{[2]}}$ and $\frac{d \text{L}}{d \mathbf{b}^{[2]}}$.
In order to calculate them, we need $\frac{d \text{L}}{d \mathbf{a}^{[2]}}$ and
$\frac{d \text{L}}{d \mathbf{z}^{[2]}}$.
</p>

<p>
Looking to the same figure above visually, and using a similar logic, it looks obvious
<b>for one sample</b>:
</p>

$$
\frac{d z^{[3]}}{d \mathbf{a}^{[2]}} = \mathbf{W}^{[3]}.\text{T}
$$

<p>
To finalize the calculation of $\frac{d \text{L}}{d \mathbf{a}^{[2]}}$:
</p>

$$
\frac{d \text{L}}{d \mathbf{a}^{[2]}} = \frac{d \text{L}}{d z^{[3]}} \frac{d z^{[3]}}{d \mathbf{a}^{[2]}}
$$

<p>
Given $\frac{d \text{L}}{d \mathbf{a}^{[2]}}$, it is easy to get
$\frac{d \text{L}}{d \mathbf{z}^{[2]}}$. We just need to apply
the gradient of <b>sigmoid</b> function.
</p>

$$
\frac{d \text{L}}{d \mathbf{z}^{[2]}} =
  \frac{d \text{L}}{d \mathbf{a}^{[2]}} \circ \frac{d \mathbf{a}^{[2]}}{d \mathbf{z}^{[2]}} =
     \frac{d \text{L}}{d \mathbf{a}^{[2]}} \circ \left
     ( \mathbf{a}^{[2]} \circ \left ( 1 - \mathbf{a}^{[2]} \right ) \right )
$$

<p>
Now, with knowing $\frac{d \text{L}}{d \mathbf{z}^{[2]}}$, we are set to calculate
$\frac{d \text{L}}{d \mathbf{W}^{[2]}}$ and $\frac{d \text{L}}{d \mathbf{b}^{[2]}}$.
Here is how we compute $\mathbf{z}^{[2]}$:
</p>

$$
\begin{matrix}
(\mathbf{W}^{[2]}) \\
\begin{bmatrix}
0 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & 0 \\
\end{bmatrix}
\end{matrix}

\begin{matrix}
(\mathbf{a}^{[1]}) \\
\begin{bmatrix}
0  \\
0  \\
0  \\
\end{bmatrix}
\end{matrix}

+

\begin{matrix}
(\mathbf{b}^{[2]})
\\
\begin{bmatrix}
0 \\
0 \\
0 \\
\end{bmatrix}
\end{matrix}

=

\begin{matrix}
(\mathbf{z}^{[2]})
\\
\begin{bmatrix}
0 \\
0 \\
0 \\
\end{bmatrix}
\end{matrix}
$$

<p>
It is easy to observe:
</p>

$$

\frac{d \mathbf{z}^{[2]}_{1}}{d \mathbf{W}^{[2]}} =
\begin{bmatrix}
  & \mathbf{a}^{[1]}.T & \\
0 & 0 & 0 \\
0 & 0 & 0 \\
\end{bmatrix},
\quad
\frac{d \mathbf{z}^{[2]}_{2}}{d \mathbf{W}^{[2]}} =
\begin{bmatrix}
0 & 0 & 0 \\
& \mathbf{a}^{[1]}.T & \\
0 & 0 & 0 \\
\end{bmatrix},
\quad
\frac{d \mathbf{z}^{[2]}_{3}}{d \mathbf{W}^{[2]}} =
\begin{bmatrix}
0 & 0 & 0 \\
0 & 0 & 0 \\
& \mathbf{a}^{[1]}.T & \\
\end{bmatrix}

$$

<p>
Applying the chain rule, we should do something like:
</p>

$$
\frac{d \text{L}}{d \mathbf{W}^{[2]}} =
  \sum_{i=1}^{3} \frac{d \text{L}}{d \mathbf{z}^{[2]}_i}
     \frac{d \mathbf{z}^{[2]}_i}{d \mathbf{W}^{[2]}}
$$

<p>
which is a more fancy way of writing:
</p>

$$
\frac{d \text{L}}{d \mathbf{W}^{[2]}} =

\frac{d \text{L}}{d \mathbf{z}^{[2]}_1}
\begin{bmatrix}
  & \mathbf{a}^{[1]}.T & \\
0 & 0 & 0 \\
0 & 0 & 0 \\
\end{bmatrix}
+
\frac{d \text{L}}{d \mathbf{z}^{[2]}_2}
\begin{bmatrix}
0 & 0 & 0 \\
  & \mathbf{a}^{[1]}.T & \\
0 & 0 & 0 \\
\end{bmatrix}
+
\frac{d \text{L}}{d \mathbf{z}^{[2]}_3}
\begin{bmatrix}
0 & 0 & 0 \\
0 & 0 & 0 \\
  & \mathbf{a}^{[1]}.T & \\
\end{bmatrix}
$$

<p>
which is idential to:
</p>

$$
\frac{d \text{L}}{d \mathbf{W}^{[2]}} =

\begin{bmatrix}
\\
 \frac{d \text{L}}{d \mathbf{z}^{[2]}} \\
\\
\end{bmatrix}

\begin{bmatrix}
 \mathbf{a}^{[1]}.T \\
\end{bmatrix}
$$

<p>
We are half-way there in terms of finishing the backprop.
</p>

<p>
We need to calcualte $\frac{d \text{L}}{d \mathbf{a}^{[1]}}$.
Let's look at $\frac{d \mathbf{z}^{[2]}}{d \mathbf{a}^{[1]}}$.
</p>

$$
\frac{d \text{L}}{\mathbf{a}^{[1]}_1} = \mathbf{W}^{[2]}_{:,1} \frac{d \text{L}}{d \mathbf{z}^{[2]}}.T, \quad
\frac{d \text{L}}{\mathbf{a}^{[1]}_2} = \mathbf{W}^{[2]}_{:,2} \frac{d \text{L}}{d \mathbf{z}^{[2]}}.T, \quad
\frac{d \text{L}}{\mathbf{a}^{[1]}_3} = \mathbf{W}^{[2]}_{:,3} \frac{d \text{L}}{d \mathbf{z}^{[2]}}.T
$$

<p>
or, equivalently:
</p>

$$
\frac{d \text{L}}{\mathbf{a}^{[1]}} = \mathbf{W}^{[2]}.T \frac{d \text{L}}{d \mathbf{z}^{[2]}}
$$

<p>
Now we only missing $\frac{dL}{dW^{(1)}}$ and $\frac{dL}{db^{(1)}}$. Let's look how we calculate $Z^{(1)}$:
</p>

$$
\begin{matrix}
(W^{(1)}) \\
\begin{bmatrix}
0 & 0  \\
0 & 0  \\
0 & 0  \\
\end{bmatrix}
\end{matrix}

\begin{matrix}
(X) \\
\begin{bmatrix}
0  \\
0  \\
\end{bmatrix}
\end{matrix}

+

\begin{matrix}
(b^{(1)})
\\
\begin{bmatrix}
0 \\
0 \\
0 \\
\end{bmatrix}
\end{matrix}

=

\begin{matrix}
(Z^{(1)})
\\
\begin{bmatrix}
0 \\
0 \\
0 \\
\end{bmatrix}
\end{matrix}
$$

<p>
Now, let's look at:
</p>

$$
\frac{dL}{dW^{(1)}_{1,:}} = \frac{dL}{dZ^{(1)}_1} X.T, \quad
\frac{dL}{dW^{(1)}_{2,:}} = \frac{dL}{dZ^{(1)}_2} X.T, \quad
\frac{dL}{dW^{(1)}_{3,:}} = \frac{dL}{dZ^{(1)}_3} X.T
$$

<p>
or equivalently:
</p>

$$
\frac{dL}{dW^{(1)}} = \frac{dL}{dZ^{(1)}} X.T
$$

<p>
Let's apply the <b>backprop</b> altogether!
</p>

# HTML
{'width': 600}
ALPHA = 0.3 # learning rate
Model = collections.namedtuple('Model', ['W1', 'b1', 'W2', 'b2', 'W3', 'b3'])

# this simple implementation is numerically unstable, because:
# np.log() returns -inf for small inputs very close to 0
def get_loss(Y, Y_hat):
  loss = -1 * np.sum(Y * np.log(Y_hat) +
                     (1-Y) * np.log(1-Y_hat))
  return loss

# semantically same with above function, and numerically stable.
def get_loss_numerically_stable(Y, Z3):
  loss = -1 * np.sum(Y * -1 * np.log(1 + np.exp(-Z3)) +
                     (1-Y) * (-Z3 - np.log(1 + np.exp(-Z3))))
  return loss

def get_gradients_matrix_ops(Z1, A1, Z2, A2, Z3, A3, Y):
  dZ3 = A3 - Y  # size (1x20)

  dW3 = (1.0/Y.size) * np.matmul(dZ3, A2.T)  # size (1x3)
  db3 = (1.0/Y.size) * np.sum(dZ3, axis = 1, keepdims = True)  # size (1x1)

  dA2 = np.matmul(W3.T, dZ3)  # size (3x20)
  dZ2 = dA2 * (A2 * (1-A2)) # size (3x20)

  dW2 = (1.0/Y.size) * np.matmul(dZ2, A1.T)  # size (3x3)
  db2 = (1.0/Y.size) * np.sum(dZ2, axis = 1, keepdims= True)  # size (3x1)

  dA1 = np.matmul(W2.T, dZ2)  # size (3x20)
  dZ1 = dA1 * (A1 * (1 - A1))  # size (3x20)

  dW1 = (1.0/Y.size) * np.matmul(dZ1, X.T)  # size (3x2)
  db1 = (1.0/Y.size) * np.sum(dZ1, axis = 1, keepdims = True) # size (3x1)

  return dW1, db1, dW2, db2, dW3, db3

def get_gradients_loops(Z1, A1, Z2, A2, Z3, A3, Y):
  dZ3 = A3 - Y  # size (1x20)

  dW3 = np.zeros((1, 3))
  db3 = np.zeros((1, 1))
  for i in range(20):
    a2 = A2[:, [i]]
    dz3 = dZ3[:, [i]]
    dW3 += (1.0/20) * np.matmul(dz3, a2.T)
    db3 += (1.0/20) * dz3

  dA2 = np.zeros((3, 20))
  for i in range(20):
    dz3 = dZ3[:, [i]]
    dA2[:, [i]] = dz3 * W3.T

  dZ2 = dA2 * (A2 * (1-A2)) # size (3x20)

  dW2 = np.zeros((3, 3))
  db2 = np.zeros((3, 1))
  for i in range(20):
    a1 = A1[:, [i]]
    dz2 = dZ2[:, [i]]
    dW2 += (1.0/20) * np.matmul(dz2, a1.T)
    db2 += (1.0/20) * dz2

  dA1 = np.zeros((3, 20))
  for i in range(20):
    dz2 = dZ2[:, [i]]
    dA1[:, [i]] = np.matmul(W2.T, dz2)

  dZ1 = dA1 * (A1 * (1-A1)) # size (3x20)

  dW1 = np.zeros((3, 2))
  db1 = np.zeros((3, 1))
  for i in range(20):
    x = X[:, [i]]
    dz1 = dZ1[:, [i]]
    dW1 += (1.0/20) * np.matmul(dz1, x.T)
    db1 += (1.0/20) * dz1

  return dW1, db1, dW2, db2, dW3, db3

def gradient_descent(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, alpha):
  W1 = W1 - alpha * dW1
  b1 = b1 - alpha * db1
  W2 = W2 - alpha * dW2
  b2 = b2 - alpha * db2
  W3 = W3 - alpha * dW3
  b3 = b3 - alpha * db3

  return W1, b1, W2, b2, W3, b3

L_cache = []
models_cache = []

for i in range(10000):
  Z1, A1, Z2, A2, Z3, A3 = forward_propagate(X, W1, b1, W2, b2, W3, b3)

  L = (1.0 / 20) * get_loss_numerically_stable(Y, Z3)

  # dW1, db1, dW2, db2, dW3, db3 = get_gradients_matrix_ops(Z1, A1, Z2, A2, Z3, A3, Y)
  dW1, db1, dW2, db2, dW3, db3 = get_gradients_loops(Z1, A1, Z2, A2, Z3, A3, Y)

  W1, b1, W2, b2, W3, b3 = gradient_descent(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, ALPHA)
  m = Model(W1, b1, W2, b2, W3, b3)

  models_cache.append(m)
  L_cache.append(L)

  if L < 0.02:
    break

plt.grid()
plt.title('Loss', size=18)
plt.xlabel('Number of iterations', size=15)
plt.ylabel('Loss', size=15)
plt.plot(L_cache)

plt.savefig('image.png')

plt.close()
plt.clf()
plt.cla()

# HTML
{}
<p>
Let's see our <b>inferences</b> on the same points using our classifier.
We are doing pretty good on inference (100%).
</p>

# HTML
{}
Z1, A1, Z2, A2, Z3, A3 = forward_propagate(X, W1, b1, W2, b2, W3, b3)
print(Y)
print(A3)


# HTML
{}
<h1> Decision Boundary </h1>

<p>
Let's try to visualize the decision boundary that error final classifier learns.
</p>

<p>
The decision boundaries of both <b>Logistic Regression</b> and <b>Softmax Regression</b>
were linear. However, the decision boundary of this lecture is not linear, for obvious reasons.
Thanks to our non-linear sigmoid function. (Please see Exercise 1)
</p>

# HTML
{'type': 'image', 'width': 600}
NX = 100
NY = 100

def plot_decision_boundary_lazy(X, Y, W1, b1, W2, b2, W3, b3):
  plt.grid()
  plt.xlim([-2.0, 2.0])
  plt.ylim([-2.0, 2.0])
  plt.xlabel('$x_1$', size=20)
  plt.ylabel('$x_2$', size=20)
  plt.title('Decision boundary - Contour plot', size = 18)

  xs = np.linspace(-2.0, 2.0, NX)
  ys = np.linspace(2.0, -2.0, NY)
  xv, yv = np.meshgrid(xs, ys)
  X_fake = np.stack((xv.flatten(), yv.flatten()), axis=0)
  _, _, _, _, _, predictions = forward_propagate(X_fake, W1, b1, W2, b2, W3, b3)

  predictions = np.stack( (1.0 - predictions, predictions, np.zeros((1, NX * NY))) )

  plt.imshow(predictions.T.reshape(NX, NY, 3), extent=[-2.0, 2.0, -2.0, 2.0])
  plt.scatter(X[0], X[1], s=50, c=colormap[Y[0]])

  plt.savefig('image.png')

plot_decision_boundary_lazy(X, Y, W1, b1, W2, b2, W3, b3)

plt.close()
plt.clf()
plt.cla()

# HTML
{}
<p>
Here, the color of the background depicts our prediction for that imaginary point. Remember that our prediction
is one dimensional and between $[0,1]$. If it is $0.0$ it means we are classifying the point confidently as
red and if it is $1.0$ we are very confident on green.
</p>

<p>
In order to create this figure, we simply convert our one dimensional prediction into three dimensions (RGB)
using the following logic. R = prediction, G = 1 - prediction, B = 0. So, for example, if our final prediction
is, say, 0.9, then the color on that point will be $[0.9, 0.1, 0]$ which is kind of dark green.
</p>

<p>
Similarly, we can plot the same as our classifier progresses through the learning process.
As you may guess, it should start from a random point and get smarter in each step.
</p>

# HTML
{'width': 600}
import matplotlib.animation as animation

NX = 100
NY = 100

xs = np.linspace(-2.0, 2.0, NX)
ys = np.linspace(2.0, -2.0, NY)
xv, yv = np.meshgrid(xs, ys)
X_fake = np.stack((xv.flatten(), yv.flatten()), axis=0)

def get_predictions(X_fake, model):
  _, _, _, _, _, predictions = forward_propagate(X_fake, *model)
  predictions = np.stack( (1.0 - predictions, predictions, np.zeros((1, NX * NY))) )
  return predictions.T.reshape(NX, NY, 3)

fig = plt.figure()

ax = fig.add_subplot(111)
ax.set_xlim([-2.0, 2.0])
ax.set_ylim([-2.0, 2.0])
ax.set_xlabel('$x_1$', size=20)
ax.set_ylabel('$x_2$', size=20)

ax.set_title('Decision boundary - Animated', size = 18)

def animate(i):
  im.set_array(get_predictions(X_fake, models_cache[i * 50]))
  text_box.set_text('Iteration: {}'.format(i * 50))
  return im, text_box

im = ax.imshow(get_predictions(X_fake, models_cache[0]), extent=[-2.0, 2.0, -2.0, 2.0], animated = True)

ax.scatter(X[0], X[1], s=50, c=colormap[Y[0]])
text_box = ax.text(0.6, 1.6, 'Iteration 0', size = 16)

anim = animation.FuncAnimation(fig, animate, len(models_cache) / 50, blit=True, interval=500)
anim.save('animation.mp4', writer='avconv', fps=6, codec="libx264")

plt.close()
plt.clf()
plt.cla()

# HTML
{}
<p>
As you can see, it starts from a random classifier that does not seem to be working well in the
beginning. And the learning process figures out where to go next to find a better classifier.
After the learning is done, the final classifier is pretty good, in fact it has 100% accuracy.
</p>

<h1> Applying Simple Neural Network using low-level Tensorflow APIs </h1>

<p>
Here is how to train the same classifier for the above red and green points using low-level TensorFlow API.
It produces <b>exact</b> same output with our own hand crafted model. Notice that it is because we start both
models from the same initial random starting point (same $W$ and same $b$) using the same <b>learning rate</b>.
</p>

# HTML
{'width': 600}
import tensorflow as tf

t_X = tf.placeholder(tf.float32, [2, None])
t_Y = tf.placeholder(tf.float32, [1, None])

# t_W1 = tf.Variable(tf.random_uniform((3, 2)))
t_W1 = tf.Variable(W1_initial.astype('f'))
t_b1 = tf.Variable(tf.zeros([3, 1]))

# t_W2 = tf.Variable(tf.random_uniform((3, 3)))
t_W2 = tf.Variable(W2_initial.astype('f'))
t_b2 = tf.Variable(tf.zeros([3, 1]))

#t_W3 = tf.Variable(tf.random_uniform((1, 3)))
t_W3 = tf.Variable(W3_initial.astype('f'))
t_b3 = tf.Variable(tf.zeros([1]))

t_Z1 = tf.matmul(t_W1, t_X) + t_b1
t_A1 = tf.sigmoid(t_Z1)

t_Z2 = tf.matmul(t_W2, t_A1) + t_b2
t_A2 = tf.sigmoid(t_Z2)

t_Z3 = tf.matmul(t_W3, t_A2) + t_b3
t_A3 = tf.sigmoid(t_Z3)

t_Loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = t_Z3,  labels = t_Y))

train = tf.train.GradientDescentOptimizer(ALPHA).minimize(t_Loss)
init = tf.global_variables_initializer()

with tf.Session() as session:
   session.run(init)
   losses = []
   for i in range(10000):
      ttrain, ttloss = session.run([train, t_Loss], feed_dict={t_X:X, t_Y:Y})
      losses.append(ttloss)

      if ttloss < 0.02:
        break

plt.grid()
plt.plot(losses)
plt.title('Tensorflow Loss', size = 18)
plt.xlabel('Number of iterations', size=15)
plt.ylabel('Loss', size=15)
plt.savefig('image.png')

plt.close()
plt.clf()
plt.cla()

# HTML
{}

<h1>Exercises</h1>
<ol>
  <li> We claim that the decision boundary of the 3-layer simple neural network as it explained
  in this chapter is not linear because using a non-linear activation function (sigmoid). However,
  we have used the same sigmoid function in Logistic Regression as well, and the decision boundary
  was linear. Also, softmax function doesn't look like linear at all. (Actually, what the heck is
  the defition (or the test) of a linear function?) And also (assuming sigmoid is a non-linear
  activation function) why we had a linear decision boundary on logistic regression but not the 3-layer
  simple neural network?
  </li>
</ol>

<h1>References</h1>

<ul>
  <li> http://karpathy.github.io/ </li>
  <li> http://colah.github.io/ </li>
  <li> https://github.com/tensorflow/workshops </li>
</ul>
