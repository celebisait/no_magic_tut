<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<html>
<head>
<title>Part2: Softmax Regression</title>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    TeX: {
       equationNumbers: { autoNumber: "AMS" },
       TagSide: "right"
    }
  });
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<link rel="stylesheet" href="pygments.css">
<link rel="stylesheet" href="style.css">
<link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
</head>

<body>

<div id="outer">
    <div id="inner">
<center><h1>Part 2: Softmax Regression <span style="color: red">[Draft]</span></h1></center>

<p>
What I cannot create, I do not understand.
-- Richard Feynman
</p>

<h1>Introduction</h1>
<p>
Let's say we want to build a model to discriminate the following <b>red</b>, <b>blue</b>
and <b>green</b> points in 2-dimensional space:
</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3</span><span class="p">,</span>  <span class="mf">0.7</span><span class="p">,</span>  <span class="mf">1.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span>
               <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3</span><span class="p">,</span>  <span class="o">-</span><span class="mf">1.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.4</span> <span class="p">,</span> <span class="o">-</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">1.4</span><span class="p">,</span>  <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span>  <span class="mf">0.4</span><span class="p">,</span>  <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">,</span>  <span class="mf">0.9</span><span class="p">,</span>
               <span class="mf">1.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.45</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4</span><span class="p">]])</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="n">colormap</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;g&#39;</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">plot_scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">colormap</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
   <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
   <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">])</span>
   <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">])</span>
   <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
   <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$x_2$&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
   <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Input 2D points&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
   <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colormap</span><span class="p">[</span><span class="n">Y</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
   <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>

<span class="n">plot_scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">colormap</span><span class="p">,</span> <span class="s1">&#39;image.png&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">cla</span><span class="p">()</span>
</pre></div>
<img class="generated_image" width="600" src="../images/image000.png"/><p>
In other words, given a point, $(x_1, x_2)$, we want to output either <b>red</b>, <b>blue</b> or <b>green</b>.
</p>

<p>
We can use <b>Softmax Regression</b> for this problem. We first learn <b>weights</b>
($w_{1,1}, w_{1,2}, w_{2,1}, w_{2,2}, w_{3,1}, w_{3,2}$) and <b>bias</b> ($b_1, b_2, b_3$).
This phase is called <b>training</b>. Then we use the following formula to predict if the
new point is red, blue or green. This phase is called <b>prediction</b> or <b>inference</b>.
</p>

<h1>One hot vector representation</h1>

<p>
We represent the output as a one hot vector. In other words, we represent <b>red points</b>
using $\begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}$ and similarly for <b>blue points</b> using
$\begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}$ and lastly for <b>green points</b> using
$\begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}$.
</p>

<h1>Computation Graph</h1>

<p>
Here is a visual representation of our model:
</p>

<img class="static_image" style="width: 900px;" src="../static_images/image003.png"/>

<p>
and simply pick the biggest $a_i$ to do the <b>final prediction</b>.
</p>

<p>
We use sigmoid function as $g(z)$:
</p>

$$ g(z) = \frac{1}{1+e^{-z}} $$

<h1>Feed-forward Phase</h1>

<p>
Let's assume that we are given the weights and bias. How do we calculate the output?
</p>

<p>
We represent $X$ as a matrix. $X$ contains all the points. In our case $X$ contains $M=20$
samples and for each sample we have $(x,y)$. $Y$ contains all the labels (red, blue and green)
as a one hot vector. $W$ has the weights. $b$ has the bias:
</p>

$$
X =
\begin{bmatrix}
    0   &  0   & \dots &  0 \\
    0   &  0   & \dots &  0 \\
\end{bmatrix}_{2 \times M}, \quad
Y =
\begin{bmatrix}
    0   &  0   & \dots &  0 \\
    0   &  0   & \dots &  0 \\
    0   &  0   & \dots &  0 \\
\end{bmatrix}_{3 \times M}, \quad
W =
\begin{bmatrix}
    0   &  0   \\
    0   &  0   \\
    0   &  0   \\
\end{bmatrix}_{3 \times 2}
b =
\begin{bmatrix}
    0  \\
    0  \\
    0  \\
\end{bmatrix}_{3 \times 1}
$$

<p>
Feed-forward basically means given $X, Y, W$ and $b$, will produce us $a$ and $L$.
</p>

$$
Z = W X + b
$$

<p>
Here we can see it visually:
</p>

$$
\begin{bmatrix}
    0   &  0   & \dots &  0 \\
    0   &  0   & \dots &  0 \\
    0   &  0   & \dots &  0 \\
\end{bmatrix}_{3 \times M}
=
\begin{bmatrix}
    0   &  0   \\
    0   &  0   \\
    0   &  0   \\
\end{bmatrix}_{3 \times 2}
\begin{bmatrix}
    0   &  0   & \dots &  0 \\
    0   &  0   & \dots &  0 \\
\end{bmatrix}_{2 \times M}
+
\begin{bmatrix}
    0  \\
    0  \\
    0  \\
\end{bmatrix}_{3 \times 1}
$$

<p>
As you may realized, the summation here is called <b>broadcasting</b>.
</p>

<p>
After getting $Z$, we want to calculate $Z'$:
</p>

$$
Z' = g(Z)
$$

<p>
Here we can see it visually:
</p>

$$
\begin{bmatrix}
    0   &  0   & \dots &  0 \\
    0   &  0   & \dots &  0 \\
    0   &  0   & \dots &  0 \\
\end{bmatrix}_{3 \times M}
=
g\left(
\begin{bmatrix}
    0   &  0   & \dots &  0 \\
    0   &  0   & \dots &  0 \\
    0   &  0   & \dots &  0 \\
\end{bmatrix}_{3 \times M}
\right)
$$

<p>
As you may realized, taking sigmoid here of a matrix mean taking the sigmoid of each
value in the matrix seperately.
</p>

<p>
Now, we need to get $a$ using $Z'$. Naively, we can do something like:
</p>

$$
A = \frac{e^{Z'}}{ \text{Sum over the rows of } e^{Z'} }
$$

Or visually:

$$
\begin{bmatrix}
    0   &  0   & \dots &  0 \\
    0   &  0   & \dots &  0 \\
    0   &  0   & \dots &  0 \\
\end{bmatrix}_{3 \times M}
=
\frac{
e^{\begin{bmatrix}
    0   &  0   & \dots &  0 \\
    0   &  0   & \dots &  0 \\
    0   &  0   & \dots &  0 \\
\end{bmatrix}_{3 \times M}}
}
{
\begin{bmatrix}
    0   &  0   & \dots &  0 \\
\end{bmatrix}_{1 \times M}
}
$$

<p>
However, this <b>numerically unstable</b>. Because $e^x$ easily overflows 64bit (even 128bit).
We need to approach it differently.
</p>

<h1>Numerical Stability of Softmax function</h1>

<p>
The Softmax function takes an N-dimensional vector of real values and
returns a new N-dimensional vector that sums up to $1$. The exact formula is (Softmax equation):
</p>

<p class="equation">
\begin{equation} \label{eq:softmax}
a_i = \frac{e^{z'_i}}{\sum_{j=1}^N e^{z'_j}}
\end{equation}
</p>

<p>
Let's make an example:
</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span>
<span class="k">print</span> <span class="n">a</span>
<span class="k">print</span> <span class="n">softmax</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</pre></div>
<div class="code_stdout"><pre>[ 1.  2.  3.]
[ 0.09003057  0.24472847  0.66524096]
</pre></div><p>
Intuitively, softmax increases/emphasizes the <b>relative difference</b> between large and small values.
</p>

<p>
However, let's look at this:
</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1000.0</span><span class="p">,</span> <span class="mf">2000.0</span><span class="p">,</span> <span class="mf">3000.0</span><span class="p">])</span>
<span class="k">print</span> <span class="n">a</span>
<span class="k">print</span> <span class="n">softmax</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</pre></div>
<div class="code_stdout"><pre>[ 1000.  2000.  3000.]
[ nan  nan  nan]
</pre></div><p>
We are getting <b>nan</b> values along with a <b>RuntimeWarning: overflow encountered in exp</b>.
Simply because:
</p>
<div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mi">1000</span><span class="p">))</span>
</pre></div>
<div class="code_stdout"><pre>inf
</pre></div>Instead, we can approach it differently:

$$
a_i = \frac{e^{z'_i}}{\sum_{j=1}^N e^{z'_j}} = \frac{e^{z'_i}e^K}{\sum_{j=1}^N e^{z'_j} e^K}
    = \frac{e^{z'_i + K}}{\sum_{j=1}^N e^{z'_j + K}}
$$

<p>
for some fixed $K$. And we can pick $K = - max(z'_1, z'_2, \dots, z'_N)$.
</p>

<p>
More practically:
</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">a</span><span class="o">-</span><span class="nb">max</span><span class="p">(</span><span class="n">a</span><span class="p">))</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">a</span><span class="o">-</span><span class="nb">max</span><span class="p">(</span><span class="n">a</span><span class="p">)))</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1000.0</span><span class="p">,</span> <span class="mf">2000.0</span><span class="p">,</span> <span class="mf">3000.0</span><span class="p">])</span>
<span class="k">print</span> <span class="n">a</span>
<span class="k">print</span> <span class="n">softmax</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</pre></div>
<div class="code_stdout"><pre>[ 1000.  2000.  3000.]
[ 0.  0.  1.]
</pre></div><p>
As you can see, we still have some numerical issues. First and second value of the softmax
shouldn't be $0$, they should be very close $0$, but not exactly $0$. Hmmm, but this is
not as bad as <b>nan</b> issue.
</p>

<p>
So, to wrap-up the Feed-Forward phase, we can finalize the forward propagation step:
</p>
<div class="highlight"><pre><span></span><span class="n">sigmoid</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">stable_softmax</span><span class="p">(</span><span class="n">Z_hat</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">Z_hat</span> <span class="o">-</span> <span class="n">Z_hat</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">))</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">Z_hat</span> <span class="o">-</span> <span class="n">Z_hat</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

<span class="c1"># this simple implementation is numerically unstable, because:</span>
<span class="c1"># np.log() returns -inf for small inputs very close to 0</span>
<span class="k">def</span> <span class="nf">get_loss</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">Y_one_hot</span><span class="p">):</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">Y_one_hot</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">A</span><span class="p">)</span> <span class="o">+</span>
                     <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">Y_one_hot</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">A</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">loss</span>

<span class="c1"># semantically same with above function, and numerically stable.</span>
<span class="k">def</span> <span class="nf">get_loss_numerically_stable</span><span class="p">(</span><span class="n">Z_hat</span><span class="p">,</span> <span class="n">Y_one_hot</span><span class="p">):</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">Y_one_hot</span> <span class="o">*</span> <span class="p">(</span> <span class="p">(</span><span class="n">Z_hat</span> <span class="o">-</span> <span class="n">Z_hat</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">))</span> <span class="o">-</span>
                                    <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">Z_hat</span> <span class="o">-</span> <span class="n">Z_hat</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">))</span>
				 <span class="p">)</span> <span class="o">+</span>
                     <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">Y_one_hot</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">Z_hat</span> <span class="o">-</span> <span class="n">Z_hat</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-</span>
		                       <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">Z_hat</span> <span class="o">-</span> <span class="n">Z_hat</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)))</span> <span class="o">-</span>
		                       <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">Z_hat</span> <span class="o">-</span> <span class="n">Z_hat</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">))</span>
				     <span class="p">)</span>
		    <span class="p">)</span>
  <span class="k">return</span> <span class="n">loss</span>

<span class="k">def</span> <span class="nf">forward_propagate</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y_one_hot</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
  <span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
  <span class="n">Z_hat</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>
  <span class="n">A</span> <span class="o">=</span> <span class="n">stable_softmax</span><span class="p">(</span><span class="n">Z_hat</span><span class="p">)</span>
  <span class="n">L1</span> <span class="o">=</span> <span class="n">get_loss</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">Y_one_hot</span><span class="p">)</span>
  <span class="n">L2</span> <span class="o">=</span> <span class="n">get_loss_numerically_stable</span><span class="p">(</span><span class="n">Z_hat</span><span class="p">,</span> <span class="n">Y_one_hot</span><span class="p">)</span>

  <span class="n">dA</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">Y</span> <span class="o">/</span> <span class="n">A</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">Y</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">A</span><span class="p">)</span>
  <span class="k">print</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

  <span class="k">print</span><span class="p">(</span><span class="n">dA</span><span class="p">)</span>

  <span class="k">print</span><span class="p">(</span><span class="n">L1</span><span class="p">)</span>
  <span class="k">print</span><span class="p">(</span><span class="n">L2</span><span class="p">)</span>

<span class="n">Y_one_hot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">3</span><span class="p">)[</span><span class="n">Y</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span><span class="o">.</span><span class="n">T</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">forward_propagate</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y_one_hot</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</pre></div>
<div class="code_stdout"><pre>(3, 20)
[[ 1.50161426  1.61799669 -3.0702384   1.58707692 -6.74846583 -2.81344576
  -2.59429925 -2.70093314 -2.63576612  1.57615739  1.51213272 -6.58715159
  -6.46241595 -6.59946465 -2.54556741  1.56032154 -2.68560773 -6.51192786
  -6.52475885 -6.46662797]
 [ 1.44404442  1.4592665  -3.15381149  1.45202514 -7.88637578 -3.19005552
  -3.15649505 -3.11858749 -3.06898856  1.44428308  1.43915755 -7.82959833
  -7.8538458  -7.7912658  -3.09393411  1.44944601 -3.14237607 -7.85737153
  -7.74494109 -7.81137494]
 [ 1.55872366  1.43538751 -2.79942872  1.46795993 -8.14852741 -3.02032863
  -3.35872112 -3.23520317 -3.39255125  1.48552971  1.55320159 -8.58654419
  -8.91821391 -8.60542834 -3.52178167  1.49434907 -3.23191298 -8.75912362
  -8.8826988  -8.96720056]]
40.0856705699
40.0856705699
</pre></div><h1>Maximum Likelihood Estimation</h1>

<p>
In training, our goal is to <b>learn</b> a matrix $W$ of size $(3 \times 2)$  and a $b$ of size $(3 \times 1)$
that best <b>discriminates</b> red, blue and green points.
</p>

<p>
We want to find $W$ and $b$ that minimizes some definition of a <b>cost function</b>.
Let's attempt to write a cost function for this problem.
</p>

<p>
Let's say we have three points:
</p>

$$x = \begin{bmatrix} -0.1 \\ 1.4 \end{bmatrix}, y=0$$

$$x = \begin{bmatrix} 1.3  \\ 0.9 \end{bmatrix}, y=1$$

<p>
and similarly,
</p>

$$x = \begin{bmatrix} -1.4  \\ -1.1 \end{bmatrix}, y=2$$

<p>
Now, let's list these $y$ as a <b>one hot vector</b> and, their corresponding $a$ values:
</p>

$$y = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, a = \begin{bmatrix} 0.9 \\ 0.1 \\ 0.0 \end{bmatrix} $$

$$y = \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}, a = \begin{bmatrix} 0.1 \\ 0.8 \\ 0.1 \end{bmatrix} $$

$$y = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}, a = \begin{bmatrix} 0.1 \\ 0.2 \\ 0.7 \end{bmatrix} $$

<p>
Intuitively, we want a classifier that produces <b>similar</b> looking $a$ and $y$. This means, if
$y = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}$, then, for example, having
$a = \begin{bmatrix} 0.8 \\ 0.1 \\ 0.1 \end{bmatrix}$ is <b>more desirable</b> than having
$a = \begin{bmatrix} 0.6 \\ 0.2 \\ 0.2 \end{bmatrix}$.

<p>
In other words, we want to <b>maximize</b>:
</p>

$$P(y|x) = \prod_{j=1}^{3} {a_j^{y_j} (1-a_j)^{(1-y_j)} }$$

<p>
Here, $a_j$ represents the jth item in the vector $a$, and similarly $y_j$ represents the jth value in $y$.
For example, when $a = \begin{bmatrix} 0.9 \\ 0.1 \\ 0.0 \end{bmatrix}$, then, $a_1 = 0.9, a_2 = 0.1$ and
$a_3 = 0.0$.
</p>

$$y = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, a = \begin{bmatrix} 0.9 \\ 0.1 \\ 0.0 \end{bmatrix}, P(y|x) = 0.9 \times 0.9 \times 1.0 = 0.810 $$

$$y = \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}, a = \begin{bmatrix} 0.1 \\ 0.8 \\ 0.1 \end{bmatrix}, P(y|x) = 0.9 \times 0.8 \times 0.9 = 0.648 $$

$$y = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}, a = \begin{bmatrix} 0.1 \\ 0.2 \\ 0.7 \end{bmatrix}, P(y|x) = 0.9 \times 0.8 \times 0.7 = 0.504 $$

<p>
<b>Bigger</b> the $P(y|x)$ is the <b>better</b>.
</p>

<p>
Similar to <b>Logistic Regression</b>, in order to define the <b>loss</b> for multiple samples, we will simply multiply
each value for each sample (<b>Maximum Likelihood Estimation</b>):
</p>

$$J = \prod_{i=1}^{M} P(y^{(i)}|x^{(i)}) $$

<p>
here, $y^{(i)}$, $x^{(i)}$ and $a^{(i)}$ corresponds to ith sample in the training set out of $M$ training samples. We can rewrite it as:
</p>

$$J = \prod_{i=1}^{M} \prod_{j=1}^{3}  (a_j^{(i)}) ^ {y_j^{(i)}} (1-a_j^{(i)})^{(1-y_j^{(i)})} $$

<p>
Maximizing above is equal to maximizing:
</p>

$$J = log \left( \prod_{i=1}^{M} \prod_{j=1}^{3} (a_j^{(i)}) ^ {y_j^{(i)}} (1-a_j^{(i)})^{(1-y_j^{(i)})} \right ) $$

<p>
with more math:
</p>

$$J = \sum_{i=1}^{M} \sum_{j=1}^{3} y_j^{(i)} log(a_j^{(i)}) (1-y_j^{(i)}) log(1-a_j^{(i)}) $$

<p>
since we like to <b>minimize</b> things instead of <b>maximizing</b>:
</p>


$$J = - \sum_{i=1}^{M} \sum_{j=1}^{3} y_j^{(i)} log(a_j^{(i)}) (1-y_j^{(i)}) log(1-a_j^{(i)}) $$

<p>
If we add our <b>Log Loss</b> to our computation graph:
</p>

<img class="static_image" style="width: 800px;" src="../static_images/image004.png"/>


<h1>Gradient Descent</h1>

<p>
Let's see what happens if we change $w_{2, 1}$ in the network, step by step:
</p>

<img class="static_image" style="width: 900px;" src="../static_images/image005.gif"/>

<p>
In order to do gradient descent, we need the derivatives:
</p>

$$
\frac{dL}{dw_{m,n}} = \sum_{i=1}^3 \left (\frac{dL}{da_i} \right) \left (\frac{da_i}{dz'_m} \right)  \left (\frac{dz'_m}{dz_m} \right ) \left(\frac{dz_m}{dw_{m,n}} \right ), \quad

\frac{dL}{db_m} = \sum_{i=1}^3 \left (\frac{dL}{da_i} \right) \left (\frac{da_i}{dz'_m} \right)  \left (\frac{dz'_m}{dz_m} \right ) \left(\frac{dz_m}{b_m} \right )
$$

<p>
Let's do some calculus:
<img src="../static_images/evil1.png" style="width:30px; height:30px;">
<img src="../static_images/evil2.png" style="width:30px; height:30px;">
</p>

$$
\frac{dL}{da_i} =  \frac{d}{da_i} \left ( - \sum_{j=1}^3 y_j log(a_j) + (1-y_j) log(1-a_j) \right ) =
\frac{d}{da_i} \left ( - y_i log(a_i) \right ) + \frac{d}{da_i} \left ( - (1-y_i) log(1-a_i) \right ) =
\frac{-y_i}{a_i} + \frac{(1-y_i)}{1-a_i}
$$

<h1>Derivative of Softmax Function</h1>

<p>
A nice property about the Softmax function that it produces a legit <b>Probability Distrubition</b>.
In classification (or more generally in Machine Learning) we often want to assign prababilities to
categories or classes. Softmax function is known to work well numereous applications/areas.
</p>


<p>
Softmax is a vector function -- it takes a vector as an input and returns another vector. Therefore, we
cannot just ask for <b>the derivative of softmax</b>, we can only ask the derivative of softmax regarding
particular elements.
</p>

<p>
For example,
</p>

$$
\frac{d}{d z'_2} a_1
$$

<p>
refers to how much $a_1$ will change if play with $z'_2$.
</p>

<p>
Using the same logic for each element for $a_i$ and $z'_j$ would produce us $N \times N$ matrix of derivatives.
</p>

<p>
Let's try to take derivative for <b>one particular element</b>:
</p>

$$
\frac{d}{d z'_m} a_i = \frac{d}{d z'_m} \frac{e^{z'_i}}{\sum_{j=1}^N e^{z'_j}}
$$

<p>
We can use <b>Quotient Rule</b> here. Recall that:
</p>

$$
\frac{d}{dx} \frac{f(x)}{g(x)} = \frac{f'(x)g(x) - g'(x)f(x)}{ [g(x)]^2 }
$$

<p>
In our case:
</p>

$$
f(x) = e^{z'_i}, \quad g(x) = \sum_{j=1}^N e^{z'_j}
$$

<p>
Let's apply <b>Quotient Rule</b>, if $i=m$:
</p>

$$
\frac{d}{d z'_m} a_i = \frac{d}{d z'_m} \frac{e^{z'_i}}{\sum_{j=1}^N e^{z'_j}} =
\frac{ (e^{z'_i})' \sum_{j=1}^N e^{z'_j}  - (\sum_{j=1}^N e^{z'_j})' e^{z'_i} }{ [\sum_{j=1}^N e^{z'_j}] ^ 2 } =
\frac{ e^{z'_i} \sum_{j=1}^N e^{z'_j} - e^{z'_m} e^{z'_i} }{ [\sum_{j=1}^N e^{z'_j}] ^ 2 } =
\frac{ e^{z'_i} } { \sum_{j=1}^N e^{z'_j} }  \frac{ \sum_{j=1}^N e^{z'_j} - e^{z'_m} } { \sum_{j=1}^N e^{z'_j} } = (a_i)(1-a_m)
$$

<p>
Notice that we simplify, by plugging $a_i$ and $a_m$ (see Equation \ref{eq:softmax}) in the last step above.
</p>

<p>
Similarly, if $i \neq m$:
</p>

$$
\frac{d}{d z'_m} a_i = \frac{d}{d z'_m} \frac{e^{z'_i}}{\sum_{j=1}^N e^{z'_j}} =
\frac{ (e^{z'_i})' \sum_{j=1}^N e^{z'_j}  - (\sum_{j=1}^N e^{z'_j})' e^{z'_i} }{ [\sum_{j=1}^N e^{z'_j}] ^ 2 } =
\frac{ 0 - e^{z'_m} e^{z'_i} }{ [\sum_{j=1}^N e^{z'_j}] ^ 2 } = -\frac{ e^{z'_m} } { \sum_{j=1}^N e^{z'_j} } \frac{ e^{z'_i} }{ \sum_{j=1}^N e^{z'_j} }
= - (a_m) (a_i)
$$

<p>
More succintly, we can summarize all above:
</p>

$$
\frac{d}{d z'_m} a_i =
  \begin{cases}
    (a_i)(1-a_m),  & \text{if}\ \quad  i = m \\
    - (a_m) (a_i), & \text{if}\ \quad  i \neq m
  \end{cases}
$$

<p>
or equally:
</p>

$$
\frac{d}{d z'_m} a_i = (a_i)(\delta_{i,m} - a_m)
$$

<p>
where $\delta_{i,m} = 1$ if $i=m$, and $0$ otherwise.
</p>

<p>
The reason we want to write it this way is that we don't want to use any <b>loops</b>. And we can
execute the above using matrix operations like:
</p>

$$
\mathbf{a} \mathbf{e^T} \circ (\mathbf{I} - \mathbf{e} \mathbf{a^T})
$$

<p>
where $\mathbf{e}$ is a vector of $1$'s of size $K\times1$ for a suitable $K$ and $\circ$ represents Hadamard product,
in other words element-wise product of matrices.
</p>

$$
\begin{bmatrix} \frac{da_1}{dz'_1} \frac{da_2}{dz'_1} \frac{da_3}{dz'_1} \\
                \frac{da_1}{dz'_2} \frac{da_2}{dz'_2} \frac{da_3}{dz'_2} \\
                \frac{da_1}{dz'_3} \frac{da_2}{dz'_3} \frac{da_3}{dz'_3}
\end{bmatrix}
\begin{bmatrix} \frac{dL}{da_1} \\
                \frac{dL}{da_2} \\
                \frac{dL}{da_3}
\end{bmatrix}
=
\begin{bmatrix} \frac{dL}{dz'_1} \\
                \frac{dL}{dz'_2} \\
                \frac{dL}{dz'_3}
\end{bmatrix}
$$

<p>
We know the derivative of sigmoid function from the previous lecture:
</p>

$$
\frac{dz'}{dz} = g(z)(1-g(z))
$$

<p>
Similarly, we also calculated (and easy to see):
</p>

$$
\frac{dz}{dw_{m,n}} = x_n, \quad \frac{dz}{db_i} = 1
$$

<p>
Now, let's combine these:
</p>


<h1>References</h1>

<ul>
  <li> http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/
  <li> https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/ </li>
  <li> http://tutorial.math.lamar.edu/Classes/CalcI/ProductQuotientRule.aspx </li>
</ul>
    </div>
</div>

</body>
</html>
