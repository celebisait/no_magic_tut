<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<html>
<head>
<title>Part2: Softmax Regression</title>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    TeX: {
       equationNumbers: { autoNumber: "AMS" },
       TagSide: "right"

    }
  });
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<link rel="stylesheet" href="pygments.css">
<link rel="stylesheet" href="style.css">
<link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
</head>

<body>

<div id="outer">
    <div id="inner">
<center><h1>Part 2: Softmax Regression <span style="color: red">[Draft]</span></h1></center>

<p>
What I cannot create, I do not understand.
-- Richard Feynman
</p>

<h1>Introduction</h1>
<p>
Let's say we want to build a model to discriminate the following <b>red</b>, <b>blue</b>
and <b>green</b> points in 2-dimensional space:
</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">suppress</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3</span><span class="p">,</span>  <span class="mf">0.7</span><span class="p">,</span>  <span class="mf">1.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span>
               <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3</span><span class="p">,</span>  <span class="o">-</span><span class="mf">1.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.4</span> <span class="p">,</span> <span class="o">-</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">1.4</span><span class="p">,</span>  <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span>  <span class="mf">0.4</span><span class="p">,</span>  <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">,</span>  <span class="mf">0.9</span><span class="p">,</span>
               <span class="mf">1.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.45</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4</span><span class="p">]])</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="n">Y_one_hot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">3</span><span class="p">)[</span><span class="n">Y</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span><span class="o">.</span><span class="n">T</span>
<span class="n">colormap</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;g&#39;</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">plot_scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">colormap</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
   <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
   <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">])</span>
   <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">])</span>
   <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
   <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$x_2$&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
   <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Input 2D points&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
   <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colormap</span><span class="p">[</span><span class="n">Y</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
   <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>

<span class="n">plot_scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">colormap</span><span class="p">,</span> <span class="s1">&#39;image.png&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">cla</span><span class="p">()</span>
</pre></div>
<img class="generated_image" width="600" src="../images/image000.png"/><p>
In other words, given a point, $(x_1, x_2)$, we want to output either <b>red</b>,
<b>blue</b> or <b>green</b>.
</p>

<p>
We can use <b>Softmax Regression</b> for this problem. We first learn <b>weights</b>
($w_{1,1}, w_{1,2}, w_{2,1}, w_{2,2}, w_{3,1}, w_{3,2}$) and <b>bias</b> ($b_1, b_2, b_3$).
This phase is called <b>training</b>. Then we use the following formula to predict if the
new point is red, blue or green. This phase is called <b>prediction</b> or <b>inference</b>.
</p>

<h1>One hot vector representation</h1>

<p>
We represent the output as a one hot vector. In other words, we represent <b>red points</b>
using $\begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}$ and similarly for <b>blue points</b> using
$\begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}$ and lastly for <b>green points</b> using
$\begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}$.
</p>

<h1>Computation Graph</h1>

<p>
Here is a visual representation of our model:
</p>

<img class="static_image" style="width: 800px;" src="../static_images/image003.png"/>

<p>
and simply pick the biggest $a_i$ to do the <b>final prediction</b>.
</p>

<h1>Feed-forward Phase</h1>

<p>
Let's assume that we are given the weights and bias. How do we calculate the output?
</p>

<p>
We represent $X$ as a matrix. $X$ contains all the points. In our case $X$ contains $M=20$
samples and for each sample we have $(x,y)$. $Y$ contains all the labels (red, blue and green)
as a one hot vector. $W$ has the weights. $b$ has the bias:
</p>

$$
X =
\begin{bmatrix}
    0   &  0   & \dots &  0 \\
    0   &  0   & \dots &  0 \\
\end{bmatrix}_{2 \times M}, \quad
Y =
\begin{bmatrix}
    0   &  0   & \dots &  0 \\
    0   &  0   & \dots &  0 \\
    0   &  0   & \dots &  0 \\
\end{bmatrix}_{3 \times M}, \quad
W =
\begin{bmatrix}
    0   &  0   \\
    0   &  0   \\
    0   &  0   \\
\end{bmatrix}_{3 \times 2}
b =
\begin{bmatrix}
    0  \\
    0  \\
    0  \\
\end{bmatrix}_{3 \times 1}
$$

<p>
Feed-forward basically means given $X, Y, W$ and $b$, will produce us $a$ and $L$.
</p>

$$
Z = W X + b
$$

<p>
Here we can see it visually:
</p>

$$
\begin{bmatrix}
    0   &  0   & \dots &  0 \\
    0   &  0   & \dots &  0 \\
    0   &  0   & \dots &  0 \\
\end{bmatrix}_{3 \times M}
=
\begin{bmatrix}
    0   &  0   \\
    0   &  0   \\
    0   &  0   \\
\end{bmatrix}_{3 \times 2}
\begin{bmatrix}
    0   &  0   & \dots &  0 \\
    0   &  0   & \dots &  0 \\
\end{bmatrix}_{2 \times M}
+
\begin{bmatrix}
    0  \\
    0  \\
    0  \\
\end{bmatrix}_{3 \times 1}
$$

<p>
As you may realized, the summation here is called <b>broadcasting</b>.
</p>

<p>
After getting $Z$, we apply softmax function over $Z$:
</p>

<p class="equation">
\begin{equation} \label{eq:softmax}
a_i = \frac{e^{z_i}}{\sum_{j=1}^N e^{z_j}}
\end{equation}
</p>

<p>
However, this may be problematic to compute for big values of $z_i$. We call this phenomena
<b>numerically unstable</b>. Because $e^{z_i}$ easily overflows 64bit (even 128bit).
We need to approach it slightly differently.
</p>

<h1>Numerical Stability of Softmax function</h1>

<p>
The Softmax function takes an N-dimensional vector of real values and
returns a new N-dimensional vector that sums up to $1$. The exact formula is (Softmax equation):
</p>

<p>
Let's make an example:
</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span>
<span class="k">print</span> <span class="n">a</span>
<span class="k">print</span> <span class="n">softmax</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</pre></div>
<div class="code_stdout"><pre>[ 1.  2.  3.]
[ 0.09   0.245  0.665]
</pre></div><p>
Intuitively, softmax increases/emphasizes the <b>relative difference</b> between large and small values.
</p>

<p>
However, let's look at this:
</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1000.0</span><span class="p">,</span> <span class="mf">2000.0</span><span class="p">,</span> <span class="mf">3000.0</span><span class="p">])</span>
<span class="k">print</span> <span class="n">a</span>
<span class="k">print</span> <span class="n">softmax</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</pre></div>
<div class="code_stdout"><pre>[ 1000.  2000.  3000.]
[ nan  nan  nan]
</pre></div><p>
We are getting <b>nan</b> values along with a <b>RuntimeWarning: overflow encountered in exp</b>.
Simply because:
</p>
<div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mi">1000</span><span class="p">))</span>
</pre></div>
<div class="code_stdout"><pre>inf
</pre></div>Instead, we can approach it differently:

$$
a_i = \frac{e^{z_i}}{\sum_{j=1}^N e^{z_j}} = \frac{e^{z_i}e^K}{\sum_{j=1}^N e^{z_j} e^K}
    = \frac{e^{z_i + K}}{\sum_{j=1}^N e^{z_j + K}}
$$

<p>
for some fixed $K$. And we can pick $K = - max(z_1, z_2, \dots, z_N)$.
</p>

<p>
More practically:
</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">a</span><span class="o">-</span><span class="nb">max</span><span class="p">(</span><span class="n">a</span><span class="p">))</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">a</span><span class="o">-</span><span class="nb">max</span><span class="p">(</span><span class="n">a</span><span class="p">)))</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1000.0</span><span class="p">,</span> <span class="mf">2000.0</span><span class="p">,</span> <span class="mf">3000.0</span><span class="p">])</span>
<span class="k">print</span> <span class="n">a</span>
<span class="k">print</span> <span class="n">softmax</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</pre></div>
<div class="code_stdout"><pre>[ 1000.  2000.  3000.]
[ 0.  0.  1.]
</pre></div><p>
As you can see, we still have some numerical issues. First and second value of the softmax
shouldn't be $0$, they should be very close $0$, but not exactly $0$. Hmmm, but this is
not as bad as <b>nan</b> issue.
</p>

<p>
So, to wrap-up the Feed-Forward phase, we can finalize the forward propagation step:
</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">stable_softmax</span><span class="p">(</span><span class="n">Z</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">Z</span> <span class="o">-</span> <span class="n">Z</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">))</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">Z</span> <span class="o">-</span> <span class="n">Z</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">forward_propagate</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y_one_hot</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
  <span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
  <span class="n">A</span> <span class="o">=</span> <span class="n">stable_softmax</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">Z</span><span class="p">,</span> <span class="n">A</span>

<span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span> <span class="mf">0.31</span><span class="p">,</span> <span class="mf">3.95</span><span class="p">],</span>
              <span class="p">[</span> <span class="mf">7.07</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.23</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mf">6.27</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.35</span><span class="p">]])</span>

<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span> <span class="mf">1.2</span>  <span class="p">],</span>
              <span class="p">[</span> <span class="mf">2.93</span> <span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mf">4.14</span> <span class="p">]])</span>

<span class="n">Z</span><span class="p">,</span> <span class="n">A</span> <span class="o">=</span> <span class="n">forward_propagate</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y_one_hot</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">Y_one_hot</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">A</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">])</span>
</pre></div>
<div class="code_stdout"><pre>[[ 1.  1.  0.  1.  0.  0.  0.  0.  0.  1.]
 [ 0.  0.  1.  0.  0.  1.  1.  1.  1.  0.]
 [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]]
[[ 0.992  0.652  0.001  0.963  0.118  0.096  0.186  0.     0.     0.988]
 [ 0.008  0.19   0.999  0.018  0.     0.904  0.734  1.     1.     0.   ]
 [ 0.     0.158  0.     0.019  0.882  0.     0.08   0.     0.     0.012]]
</pre></div><p>
Above we print the first 10 predictions, and they look pretty good. In fact, we have 100% accuracy.
So, given the weights, and bias, it is pretty straight-forward to calculate the final predictions.
The tricky part is to <i>learn</i> those weights properly.
</p>
<h1>Defining Loss function (Maximum Likelihood Estimation)</h1>

<p>
In training, our goal is to <b>learn</b> a matrix $W$ of size $(3 \times 2)$  and a $\mathbf{b}$ of size $(3 \times 1)$
that best <b>discriminates</b> red, blue and green points.
</p>

<p>
We want to find $W$ and $\mathbf{b}$ that minimizes some definition of a <b>cost function</b>.
Let's attempt to write a cost function for this problem.
</p>

<p>
Let's say we have three points:
</p>

$$\mathbf{x} = \begin{bmatrix} -0.1 \\ 1.4 \end{bmatrix}, y=0$$

$$\mathbf{x} = \begin{bmatrix} 1.3  \\ 0.9 \end{bmatrix}, y=1$$

<p>
and similarly,
</p>

$$\mathbf{x} = \begin{bmatrix} -1.4  \\ -1.1 \end{bmatrix}, y=2$$

<p>
Now, let's list these $y$ as a <b>one hot vector</b> and, their corresponding <i>imaginary</i> $\mathbf{a}$ values:
</p>

$$\mathbf{y} = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, \mathbf{a} = \begin{bmatrix} 0.9 \\ 0.1 \\ 0.0 \end{bmatrix} $$

$$\mathbf{y} = \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}, \mathbf{a} = \begin{bmatrix} 0.1 \\ 0.8 \\ 0.1 \end{bmatrix} $$

$$\mathbf{y} = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}, \mathbf{a} = \begin{bmatrix} 0.1 \\ 0.2 \\ 0.7 \end{bmatrix} $$

<p>
Intuitively, we want a classifier that produces <b>similar</b> looking $\mathbf{a}$ and $\mathbf{y}$.
This means, if
$\mathbf{y} = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}$, then, for example, having
$\mathbf{a} = \begin{bmatrix} 0.8 \\ 0.1 \\ 0.1 \end{bmatrix}$ is <b>more desirable</b> than having
$\mathbf{a} = \begin{bmatrix} 0.6 \\ 0.2 \\ 0.2 \end{bmatrix}$.

<p>
In other words, we want to <b>maximize</b>:
</p>

$$P(\mathbf{y}|\mathbf{x}) = \prod_{j=1}^{3} a_j^{y_j} $$

<p>
Here, $a_j$ represents the jth item in the vector $\mathbf{a}$, and similarly $y_j$ represents the jth value in $\mathbf{y}$.
For example, when $\mathbf{a} = \begin{bmatrix} 0.9 \\ 0.1 \\ 0.0 \end{bmatrix}$, then, $a_1 = 0.9, a_2 = 0.1$ and
$a_3 = 0.0$.
</p>

$$\mathbf{y} = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, a = \begin{bmatrix} 0.9 \\ 0.1 \\ 0.0 \end{bmatrix},
  P(\mathbf{y}|\mathbf{x}) = 0.9 \times 1 \times 1 = 0.9 $$

$$\mathbf{y} = \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}, a = \begin{bmatrix} 0.1 \\ 0.8 \\ 0.1 \end{bmatrix},
  P(\mathbf{y}|\mathbf{x}) = 1 \times 0.8 \times 1 = 0.8 $$

$$\mathbf{y} = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}, a = \begin{bmatrix} 0.1 \\ 0.2 \\ 0.7 \end{bmatrix},
  P(\mathbf{y}|\mathbf{x}) = 1 \times 1 \times 0.7 = 0.7 $$

<p>
<b>Bigger</b> the $P(\mathbf{y}|\mathbf{x})$ is the <b>better</b>.
</p>

<p>
Similar to <b>Logistic Regression</b>, in order to define the <b>loss</b> for multiple samples, we will simply multiply
each value for each sample (<b>Maximum Likelihood Estimation</b>):
</p>

$$J = \prod_{i=1}^{M} P(y^{(i)}|x^{(i)}) $$

<p>
here, $y^{(i)}$, $x^{(i)}$ and $a^{(i)}$ corresponds to ith sample in the training set out of $M$ training samples. We can rewrite it as:
</p>

$$J = \prod_{i=1}^{M} \prod_{j=1}^{3}  (a_j^{(i)}) ^ {y_j^{(i)}} $$

<p>
Maximizing above is equal to maximizing:
</p>

$$J = log \left( \prod_{i=1}^{M} \prod_{j=1}^{3} (a_j^{(i)}) ^ {y_j^{(i)}} \right ) $$

<p>
we can write it as:
</p>

$$J = \sum_{i=1}^{M} \sum_{j=1}^{3} y_j^{(i)} log(a_j^{(i)}) $$

<p>
since we like to <b>minimize</b> things instead of <b>maximizing</b>:
</p>

$$J = - \sum_{i=1}^{M} \sum_{j=1}^{3} y_j^{(i)} log(a_j^{(i)}) $$

<p>
If we add our <b>Log Loss</b> to our computation graph:
</p>

<img class="static_image" style="width: 900px;" src="../static_images/image004.png"/>

<h1>Gradient Descent</h1>

<p>
Ideally, we want to start with <b>random</b> parameters and make our parameters
better and better gradually as an iterative manner. Gradient descent is simply:
</p>

$$
W = W - \alpha \frac{dL}{dW} \quad \mathbf{b} = \mathbf{b} - \alpha \frac{dL}{d \mathbf{b}}
$$

<p>
The tricky part here is to compute $\frac{dL}{dW}$ and $\frac{dL}{d \mathbf{b}}$. We need
to do a small scale <b>back propagation</b> of derivatives here.
</p>

<p>
But first, let's see what happens if we change $w_{2, 1}$ in the network, step by step:
</p>

<p>
<center>
UPDATED ANIMATION COMES HERE. <br/>
UPDATED ANIMATION COMES HERE. <br/>
UPDATED ANIMATION COMES HERE. <br/>
</center>
</p>

<p>
In order to do gradient descent, we need the derivatives:
</p>

$$
\frac{dL}{dw_{m,n}} = \sum_{i=1}^3 \left (\frac{dL}{da_i} \right) \left (\frac{da_i}{dz_m} \right) \left(\frac{dz_m}{dw_{m,n}} \right ), \quad

\frac{dL}{db_m} = \sum_{i=1}^3 \left (\frac{dL}{da_i} \right) \left (\frac{da_i}{dz_m} \right) \left(\frac{dz_m}{b_m} \right )
$$

<p>
Let's do some calculus:
<img src="../static_images/evil1.png" style="width:30px; height:30px;">
<img src="../static_images/evil2.png" style="width:30px; height:30px;">
</p>

$$
\frac{dL}{da_i} =  \frac{d}{da_i} \left ( - \sum_{j=1}^3 y_j log(a_j) \right ) =
\frac{d}{da_i} \left ( - y_i log(a_i) \right ) = \frac{-y_i}{a_i}
$$

$$
\frac{dz_m}{dw_{m,n}} = x_n, \quad
\frac{dz_m}{b_m} = 1
$$

<h1>Derivative of Softmax Function</h1>

<p>
A nice property about the Softmax function that it produces a legit <b>Probability Distrubition</b>.
In classification (or more generally in Machine Learning) we often want to assign prababilities to
categories or classes. Softmax function is known to work well numereous applications/areas.
</p>


<p>
Softmax is a vector function -- it takes a vector as an input and returns another vector. Therefore, we
cannot just ask for <b>the derivative of softmax</b>, we can only ask the derivative of softmax regarding
particular elements.
</p>

<p>
For example,
</p>

$$
\frac{d}{d z_2} a_1
$$

<p>
refers to how much $a_1$ will change if play with $z_2$.
</p>

<p>
Using the same logic for each element for $a_i$ and $z_j$ would produce us $N \times N$ matrix of derivatives.
</p>

<p>
Let's try to take derivative for <b>one particular element</b>:
</p>

$$
\frac{d}{d z_m} a_i = \frac{d}{d z_m} \frac{e^{z_i}}{\sum_{j=1}^N e^{z_j}}
$$

<p>
We can use <b>Quotient Rule</b> here. Recall that:
</p>

$$
\frac{d}{dx} \frac{f(x)}{g(x)} = \frac{f'(x)g(x) - g'(x)f(x)}{ [g(x)]^2 }
$$

<p>
In our case:
</p>

$$
f(x) = e^{z_i}, \quad g(x) = \sum_{j=1}^N e^{z_j}
$$

<p>
Let's apply <b>Quotient Rule</b>, if $i=m$:
</p>

$$
\frac{d}{d z_m} a_i = \frac{d}{d z_m} \frac{e^{z_i}}{\sum_{j=1}^N e^{z_j}} =
\frac{ (e^{z_i})' \sum_{j=1}^N e^{z_j}  - (\sum_{j=1}^N e^{z_j})' e^{z_i} }{ [\sum_{j=1}^N e^{z_j}] ^ 2 } =
\frac{ e^{z_i} \sum_{j=1}^N e^{z_j} - e^{z_m} e^{z_i} }{ [\sum_{j=1}^N e^{z_j}] ^ 2 } =
\frac{ e^{z_i} } { \sum_{j=1}^N e^{z_j} }  \frac{ \sum_{j=1}^N e^{z_j} - e^{z_m} } { \sum_{j=1}^N e^{z_j} } = (a_i)(1-a_m)
$$

<p>
Notice that we simplify, by plugging $a_i$ and $a_m$ (see Equation \ref{eq:softmax}) in the last step above.
</p>

<p>
Similarly, if $i \neq m$:
</p>

$$
\frac{d}{d z_m} a_i = \frac{d}{d z_m} \frac{e^{z_i}}{\sum_{j=1}^N e^{z_j}} =
\frac{ (e^{z_i})' \sum_{j=1}^N e^{z_j}  - (\sum_{j=1}^N e^{z_j})' e^{z_i} }{ [\sum_{j=1}^N e^{z_j}] ^ 2 } =
\frac{ 0 - e^{z_m} e^{z_i} }{ [\sum_{j=1}^N e^{z_j}] ^ 2 } = -\frac{ e^{z_m} } { \sum_{j=1}^N e^{z_j} } \frac{ e^{z_i} }{ \sum_{j=1}^N e^{z_j} }
= - (a_m) (a_i)
$$

<p>
More succintly, we can summarize all above:
</p>

$$
\frac{d}{d z_m} a_i =
  \begin{cases}
    (a_i)(1-a_m),  & \text{if}\ \quad  i = m \\
    - (a_m) (a_i), & \text{if}\ \quad  i \neq m
  \end{cases}
$$

<p>
or equally:
</p>

$$
\frac{d}{d z_m} a_i = (a_i)(\delta_{i,m} - a_m)
$$

<p>
where $\delta_{i,m} = 1$ if $i=m$, and $0$ otherwise.
</p>

<p>
The reason we want to write it this way is that we don't want to use any <b>loops</b>. And we can
execute the above using matrix operations like:
</p>

$$
\frac{d}{dz} a = \mathbf{a} \mathbf{e^T} \circ (\mathbf{I} - \mathbf{e} \mathbf{a^T})
$$

<p>
where $\mathbf{e}$ is a vector of $1$'s of size $K\times1$ for a suitable $K$ and $\circ$ represents Hadamard product,
in other words element-wise product of matrices. And it is of size $(3 \times 3)$. We plug this below:
</p>

$$
\begin{bmatrix} \frac{da_1}{dz_1} \frac{da_2}{dz_1} \frac{da_3}{dz_1} \\
                \frac{da_1}{dz_2} \frac{da_2}{dz_2} \frac{da_3}{dz_2} \\
                \frac{da_1}{dz_3} \frac{da_2}{dz_3} \frac{da_3}{dz_3}
\end{bmatrix}_{3 \times 3}
\begin{bmatrix} \frac{dL}{da_1} \\
                \frac{dL}{da_2} \\
                \frac{dL}{da_3}
\end{bmatrix}_{3 \times 1}
=
\begin{bmatrix} \frac{dL}{dz_1} \\
                \frac{dL}{dz_2} \\
                \frac{dL}{dz_3}
\end{bmatrix}_{3 \times 1}
$$

<p>
This is exactly what we are going to do. However, this is defined for only one sample. We don't want
to loop over each sample and compute this sequantially. Ideally, we want to do everything using
matrix operations including this step. So, let's attempt to convert this to matrix operations.
</p>

<h1> Calculation using Matrix Operations </h1>

<p>
Now, instead of $\mathbf{a}$, we have $A$:
</p>

$$
A =
\left[
\begin{array}{c|c|c|c}
a_1 & a_2 & \dots & a_{20} \\
\end{array}
\right]_{3 \times 20}
$$

<p>
and we want to do:
</p>

$$
\left[
\begin{array}{c|c|c|c}
\mathcal{A}_1 & \mathcal{A}_2 & \dots & \mathcal{A}_{20} \\
\end{array}
\right]_{3 \times (3 \times 20)}
\left[
\begin{array}{c|c|c|c}
\frac{dL}{da^{(1)}} & 0                 & \dots  & 0                  \\ \hline
0                 & \frac{dL}{da^{(2)}} & \dots  & 0                  \\ \hline
\vdots            & \vdots            & \vdots & \vdots             \\ \hline
0                 & 0                 & \dots  & \frac{dL}{da^{(20)}} \\
\end{array}
\right]_{(3 \times 20) \times 20}
=
\left[
\begin{array}{c|c|c|c}
\frac{dL}{dz^{(1)}} & \frac{dL}{dz^{(2)}} & \dots & \frac{dL}{dz^{(20)}}
\end{array}
\right]_{3 \times 20}
$$

<p>
where each $\mathcal{A}_i$ of size $(3 \times 3)$ and each $\frac{dL}{da^{(i)}}$ of size $(3 \times 1)$.
And finally we define:
</p>

$$
\mathcal{A}_{3 \times (3 \times 20)} =
\left[
\begin{array}{c|c|c|c}
a_1 & a_2 & \dots & a_{20} \\
\end{array}
\right]_{3 \times 20}
\left[
\begin{array}{c|c|c|c|c}
e & 0 & 0 & \dots & 0 \\ \hline
0 & e & 0 & \dots & 0 \\ \hline
\vdots & \vdots & \vdots & \vdots & \vdots \\ \hline
0 & 0 & 0 & \dots & e \\
\end{array}
\right]_{20 \times (3 \times 20)}
$$

<p>
where all $e$ of size $(1 \times 3)$.
</p>



<h1> Back propagation Phase </h1>

<p>
We will <i>learn</i> the weights using <b>Back Propagation</b>.
</p>
<div class="highlight"><pre><span></span><span class="n">ALPHA</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="c1"># learning rate</span>

<span class="c1"># this simple implementation is numerically unstable, because:</span>
<span class="c1"># np.log() returns -inf for small inputs very close to 0</span>
<span class="k">def</span> <span class="nf">get_loss</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">Y_one_hot</span><span class="p">):</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">Y_one_hot</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">A</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">loss</span>

<span class="c1"># semantically same with above function, and numerically stable.</span>
<span class="k">def</span> <span class="nf">get_loss_numerically_stable</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">Y_one_hot</span><span class="p">):</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">Y_one_hot</span> <span class="o">*</span> <span class="p">(</span> <span class="p">(</span><span class="n">Z</span> <span class="o">-</span> <span class="n">Z</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">))</span> <span class="o">-</span>
                                    <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">Z</span> <span class="o">-</span> <span class="n">Z</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">))</span>
				 <span class="p">))</span>
  <span class="k">return</span> <span class="n">loss</span>

<span class="k">def</span> <span class="nf">get_gradients</span><span class="p">(</span><span class="n">Y_one_hot</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">A</span><span class="p">):</span>
  <span class="n">L</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="mi">20</span><span class="p">)</span> <span class="o">*</span> <span class="n">get_loss_numerically_stable</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">Y_one_hot</span><span class="p">)</span>

  <span class="n">dA</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">Y_one_hot</span> <span class="o">/</span> <span class="n">A</span><span class="p">)</span>
  <span class="n">dZ</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">A</span><span class="p">[:,</span> <span class="p">[</span><span class="n">i</span><span class="p">]]</span>
    <span class="n">da</span> <span class="o">=</span> <span class="n">dA</span><span class="p">[:,</span> <span class="p">[</span><span class="n">i</span><span class="p">]]</span>
    <span class="n">temp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">a</span><span class="o">.</span><span class="n">T</span><span class="p">))</span>
    <span class="n">dZ</span><span class="p">[:,</span> <span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">temp</span><span class="p">,</span> <span class="n">da</span><span class="p">)</span>

  <span class="n">dW</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
  <span class="n">db</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="p">[</span><span class="n">i</span><span class="p">]]</span><span class="o">.</span><span class="n">T</span>
    <span class="n">dz</span> <span class="o">=</span> <span class="n">dZ</span><span class="p">[:,</span> <span class="p">[</span><span class="n">i</span><span class="p">]]</span>
    <span class="n">dw</span> <span class="o">=</span> <span class="n">dz</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">x</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">x</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">dW</span> <span class="o">+=</span> <span class="n">dw</span>
    <span class="n">db</span> <span class="o">+=</span> <span class="n">dz</span>

  <span class="n">dW</span> <span class="o">=</span> <span class="n">dW</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span><span class="o">/</span><span class="mi">20</span><span class="p">)</span>
  <span class="n">db</span> <span class="o">=</span> <span class="n">db</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span><span class="o">/</span><span class="mi">20</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">dW</span><span class="p">,</span> <span class="n">db</span><span class="p">,</span> <span class="n">L</span>

<span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">dW</span><span class="p">,</span> <span class="n">db</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
  <span class="n">W</span> <span class="o">=</span> <span class="n">W</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">dW</span>
  <span class="n">b</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">db</span>
  <span class="k">return</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span>

<span class="c1"># initialization</span>
<span class="n">Y_one_hot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">3</span><span class="p">)[</span><span class="n">Y</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span><span class="o">.</span><span class="n">T</span> <span class="o">*</span> <span class="mf">1.0</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
  <span class="n">Z</span><span class="p">,</span> <span class="n">A</span> <span class="o">=</span> <span class="n">forward_propagate</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y_one_hot</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
  <span class="n">dW</span><span class="p">,</span> <span class="n">db</span><span class="p">,</span> <span class="n">L</span> <span class="o">=</span> <span class="n">get_gradients</span><span class="p">(</span><span class="n">Y_one_hot</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">A</span><span class="p">)</span>
  <span class="n">W</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">dW</span><span class="p">,</span> <span class="n">db</span><span class="p">,</span> <span class="n">ALPHA</span><span class="p">)</span>
  <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">L</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Number of iterations&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;image.png&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">cla</span><span class="p">()</span>
</pre></div>
<img class="generated_image" width="600" src="../images/image001.png"/><h1> Applying Softmax Regression using low-level Tensorflow APIs </h1>

<p>
Here is how to train the same classifier for the above red, blue, and green points using low-level TensorFlow API:
</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>

<span class="n">t_X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">t_Y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>

<span class="n">t_W</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_uniform</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span>
<span class="n">t_b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span>

<span class="n">t_logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">t_X</span><span class="p">,</span> <span class="n">t_W</span><span class="p">)</span> <span class="o">+</span> <span class="n">t_b</span>
<span class="n">t_Softmax</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">t_logits</span><span class="p">)</span>
<span class="n">t_Accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy</span><span class="p">(</span><span class="n">labels</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">t_Y</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
                                         <span class="n">predictions</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">t_Softmax</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

<span class="n">t_Loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax_cross_entropy_with_logits</span><span class="p">(</span>
                         <span class="n">labels</span><span class="o">=</span><span class="n">t_Y</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">t_logits</span><span class="p">))</span>

<span class="n">train</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">t_Loss</span><span class="p">)</span>
<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>

<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">accs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">session</span><span class="p">:</span>
   <span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>
   <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
      <span class="n">ttrain</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">acc</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">train</span><span class="p">,</span> <span class="n">t_Loss</span><span class="p">,</span> <span class="n">t_Accuracy</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">t_X</span><span class="p">:</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">t_Y</span><span class="p">:</span><span class="n">Y_one_hot</span><span class="o">.</span><span class="n">T</span><span class="p">})</span>
      <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
      <span class="n">accs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">acc</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Number of iterations&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;image.png&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">cla</span><span class="p">()</span>
</pre></div>
<img class="generated_image" width="600" src="../images/image002.png"/><h1>References</h1>

<ul>
  <li> http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/
  <li> https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/ </li>
  <li> http://tutorial.math.lamar.edu/Classes/CalcI/ProductQuotientRule.aspx </li>
</ul>
    </div>
</div>

</body>
</html>
