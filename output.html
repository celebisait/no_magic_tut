<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<html>
<head>
<title>Part 1: Logistic Regression</title>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    TeX: {
       equationNumbers: { autoNumber: "AMS" },
       TagSide: "right"
    }
  });
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<link rel="stylesheet" href="pygments.css">
<link rel="stylesheet" href="style.css">
<link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
</head>

<body>

<div id="outer">
    <div id="inner">
<center><h1>Part 2: Softmax Regression <span style="color: red">[Draft]</span></h1></center>

<h1>Introduction</h1>
<p>
Let's say we want to build a model to discriminate the following <b>red</b>, <b>blue</b>
and <b>green</b> points in 2-dimensional space:
</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3</span><span class="p">,</span>  <span class="mf">0.7</span><span class="p">,</span>  <span class="mf">1.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span>
               <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3</span><span class="p">,</span>  <span class="o">-</span><span class="mf">1.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.4</span> <span class="p">,</span> <span class="o">-</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">1.4</span><span class="p">,</span>  <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span>  <span class="mf">0.4</span><span class="p">,</span>  <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">,</span>  <span class="mf">0.9</span><span class="p">,</span>
               <span class="mf">1.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.45</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4</span><span class="p">]])</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="n">colormap</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;g&#39;</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">plot_scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">colormap</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
   <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
   <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">])</span>
   <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">])</span>
   <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
   <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$x_2$&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
   <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Input 2D points&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
   <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colormap</span><span class="p">[</span><span class="n">Y</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
   <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>

<span class="n">plot_scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">colormap</span><span class="p">,</span> <span class="s1">&#39;image.png&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">cla</span><span class="p">()</span>
</pre></div>
<img class="generated_image" width="600" src="images/image000.png"/><p>
In other words, given a point, $(x_1, x_2)$, we want to output either <b>red</b>, <b>blue</b> or <b>green</b>.
</p>

<p>
We can use <b>Softmax Regression</b> for this problem. We first learn <b>weights</b>
($w_{1,1}, w_{1,2}, w_{2,1}, w_{2,2}, w_{3,1}, w_{3,2}$) and <b>bias</b> ($b_1, b_2, b_3$).
This phase is called <b>training</b>. Then we use the following formula to predict if the
new point is red, blue or green. This phase is called <b>prediction</b> or <b>inference</b>.
</p>

<h1>One hot vector representation</h1>

<p>
We represent the output as a one hot vector. In other words, we represent <b>red points</b>
using $\begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}$ and similarly for <b>blue points</b> using
$\begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}$ and lastly for <b>green points</b> using
$\begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}$.
</p>

<h1>Computation Graph</h1>

<p>
Here is a visual representation of our model:
</p>

<img class="static_image" style="width: 900px;" src="static_images/image003.png"/>

<p>
and simply pick the biggest $a_i$ to do the <b>final prediction</b>.
</p>

<p>
We use sigmoid function as $g(z)$:
</p>

$$ g(z) = \frac{1}{1+e^{-z}} $$

<h1>Maximum Likelihood Estimation</h1>

<p>
In training, our goal is to <b>learn</b> a matrix $W$ of size $(3 \times 2)$  and a $b$ of size $(3 \times 1)$
that best <b>discriminates</b> red, blue and green points.
</p>

<p>
We want to find $W$ and $b$ that minimizes some definition of a <b>cost function</b>.
Let's attempt to write a cost function for this problem.
</p>

<p>
Let's say we have three points:
</p>

$$x = \begin{bmatrix} -0.1 \\ 1.4 \end{bmatrix}, y=0$$

$$x = \begin{bmatrix} 1.3  \\ 0.9 \end{bmatrix}, y=1$$

<p>
and similarly,
</p>

$$x = \begin{bmatrix} -1.4  \\ -1.1 \end{bmatrix}, y=2$$

<p>
Now, let's list these $y$ as a <b>one hot vector</b> and, their corresponding $a$ values:
</p>

$$y = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, a = \begin{bmatrix} 0.9 \\ 0.1 \\ 0.0 \end{bmatrix} $$

$$y = \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}, a = \begin{bmatrix} 0.1 \\ 0.8 \\ 0.1 \end{bmatrix} $$

$$y = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}, a = \begin{bmatrix} 0.1 \\ 0.2 \\ 0.7 \end{bmatrix} $$

<p>
Intuitively, we want a classifier that produces <b>similar</b> looking $a$ and $y$. This means, if
$y = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}$, then, for example, having
$a = \begin{bmatrix} 0.8 \\ 0.1 \\ 0.1 \end{bmatrix}$ is <b>more desirable</b> than having
$a = \begin{bmatrix} 0.6 \\ 0.2 \\ 0.2 \end{bmatrix}$.

<p>
In other words, we want to <b>maximize</b>:
</p>

$$P(y|x) = \prod_{j=1}^{3} {a_j^{y_j} (1-a_j)^{(1-y_j)} }$$

<p>
Here, $a_j$ represents the jth item in the vector $a$, and similarly $y_j$ represents the jth value in $y$.
For example, when $a = \begin{bmatrix} 0.9 \\ 0.1 \\ 0.0 \end{bmatrix}$, then, $a_1 = 0.9, a_2 = 0.1$ and
$a_3 = 0.0$.
</p>

$$y = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, a = \begin{bmatrix} 0.9 \\ 0.1 \\ 0.0 \end{bmatrix}, P(y|x) = 0.9 \times 0.9 \times 1.0 = 0.810 $$

$$y = \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}, a = \begin{bmatrix} 0.1 \\ 0.8 \\ 0.1 \end{bmatrix}, P(y|x) = 0.9 \times 0.8 \times 0.9 = 0.648 $$

$$y = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}, a = \begin{bmatrix} 0.1 \\ 0.2 \\ 0.7 \end{bmatrix}, P(y|x) = 0.9 \times 0.8 \times 0.7 = 0.504 $$

<p>
<b>Bigger</b> the $P(y|x)$ is the <b>better</b>.
</p>

<p>
Similar to <b>Logistic Regression</b>, in order to define the <b>loss</b> for multiple samples, we will simply multiply
each value for each sample (<b>Maximum Likelihood Estimation</b>):
</p>

$$J = \prod_{i=1}^{M} P(y^{(i)}|x^{(i)}) $$

<p>
here, $y^{(i)}$, $x^{(i)}$ and $a^{(i)}$ corresponds to ith sample in the training set out of $M$ training samples. We can rewrite it as:
</p>

$$J = \prod_{i=1}^{M} \prod_{j=1}^{3}  (a_j^{(i)}) ^ {y_j^{(i)}} (1-a_j^{(i)})^{(1-y_j^{(i)})} $$

<p>
Maximizing above is equal to maximizing:
</p>

$$J = log \left( \prod_{i=1}^{M} \prod_{j=1}^{3} (a_j^{(i)}) ^ {y_j^{(i)}} (1-a_j^{(i)})^{(1-y_j^{(i)})} \right ) $$

<p>
with more math:
</p>

$$J = \sum_{i=1}^{M} \sum_{j=1}^{3} y_j^{(i)} log(a_j^{(i)}) (1-y_j^{(i)}) log(1-a_j^{(i)}) $$

<p>
If we add our <b>Log Loss</b> to our computation graph:
</p>

<img class="static_image" style="width: 800px;" src="static_images/image004.png"/>


<h1>Gradient Descent</h1>

<p>
In order to do gradient descent, we need the derivatives:
</p>

$$
\frac{dL}{dW} = \frac{dL}{da} \frac{da}{dz'} \frac{dz'}{dz} \frac{dz}{dW}, \quad

\frac{dL}{db} = \frac{dL}{da} \frac{da}{dz'} \frac{dz'}{dz} \frac{dz}{db}
$$

<p>
Let's do some calculus:
<img src="static_images/evil1.png" style="width:30px; height:30px;">
<img src="static_images/evil2.png" style="width:30px; height:30px;">
</p>

<p>
TODO TODO TODO TODO. Actually do the math.
</p>
<h1>References</h1>

<ul>
  <li> http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/
</ul>
    </div>
</div>

</body>
</html>
