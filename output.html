<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<html>
<head>
<title>Part 1: Logistic Regression</title>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    TeX: {
       equationNumbers: { autoNumber: "AMS" },
       TagSide: "right"
    }
  });
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<link rel="stylesheet" href="pygments.css">
<link rel="stylesheet" href="style.css">
<link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
</head>

<body>

<div id="outer">
    <div id="inner">
<center><h1>Part 2: Softmax Regression <span style="color: red">[Draft]</span></h1></center>

<h1>Introduction</h1>
<p>
Let's say we want to build a model to discriminate the following <b>red</b>, <b>blue</b>
and <b>green</b> points in 2-dimensional space:
</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3</span><span class="p">,</span>  <span class="mf">0.7</span><span class="p">,</span>  <span class="mf">1.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span>
               <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3</span><span class="p">,</span>  <span class="o">-</span><span class="mf">1.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.4</span> <span class="p">,</span> <span class="o">-</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">1.4</span><span class="p">,</span>  <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span>  <span class="mf">0.4</span><span class="p">,</span>  <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">,</span>  <span class="mf">0.9</span><span class="p">,</span>
               <span class="mf">1.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.45</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4</span><span class="p">]])</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="n">colormap</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;g&#39;</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">plot_scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">colormap</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
   <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
   <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">])</span>
   <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">])</span>
   <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
   <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$x_2$&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
   <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Input 2D points&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
   <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colormap</span><span class="p">[</span><span class="n">Y</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
   <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>

<span class="n">plot_scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">colormap</span><span class="p">,</span> <span class="s1">&#39;image.png&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">cla</span><span class="p">()</span>
</pre></div>
<img class="generated_image" width="600" src="images/image000.png"/><p>
In other words, given a point, $(x_1, x_2)$, we want to output either <b>red</b>, <b>blue</b> or <b>green</b>.
</p>

<p>
We can use <b>Softmax Regression</b> for this problem. We first learn <b>weights</b>
($w_{1,1}, w_{1,2}, w_{2,1}, w_{2,2}, w_{3,1}, w_{3,2}$) and <b>bias</b> ($b_1, b_2, b_3$).
This phase is called <b>training</b>. Then we use the following formula to predict if the
new point is red, blue or green. This phase is called <b>prediction</b> or <b>inference</b>.
</p>

<h1>One hot vector representation</h1>

<p>
We represent the output as a one hot vector. In other words, we represent <b>red points</b>
using $\begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}$ and similarly for <b>blue points</b> using
$\begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}$ and lastly for <b>green points</b> using
$\begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}$.
</p>

<h1>Computation Graph</h1>

<p>
Here is a visual representation of our model:
</p>

<img class="static_image" style="width: 900px;" src="static_images/image003.png"/>

<p>
and simply pick the biggest $a_i$ to do the <b>final prediction</b>.
</p>

<p>
We use sigmoid function as $g(z)$:
</p>

$$ g(z) = \frac{1}{1+e^{-z}} $$

<h1>Maximum Likelihood Estimation</h1>

<p>
In training, our goal is to <b>learn</b> a matrix $W$ of size $(3 \times 2)$  and a $b$ of size $(3 \times 1)$
that best <b>discriminates</b> red, blue and green points.
</p>

<p>
We want to find $W$ and $b$ that minimizes some definition of a <b>cost function</b>.
Let's attempt to write a cost function for this problem.
</p>

<p>
Let's say we have three points:
</p>

$$x = \begin{bmatrix} -0.1 \\ 1.4 \end{bmatrix}, y=0$$

$$x = \begin{bmatrix} 1.3  \\ 0.9 \end{bmatrix}, y=1$$

<p>
and similarly,
</p>

$$x = \begin{bmatrix} -1.4  \\ -1.1 \end{bmatrix}, y=2$$

<p>
Now, let's list these $y$ as a <b>one hot vector</b> and, their corresponding $a$ values:
</p>

$$y = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, a = \begin{bmatrix} 0.9 \\ 0.1 \\ 0.0 \end{bmatrix} $$

$$y = \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}, a = \begin{bmatrix} 0.1 \\ 0.8 \\ 0.1 \end{bmatrix} $$

$$y = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}, a = \begin{bmatrix} 0.1 \\ 0.2 \\ 0.7 \end{bmatrix} $$

<p>
Intuitively, we want a classifier that produces <b>similar</b> looking $a$ and $y$. This means, if
$y = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}$, then, for example, having
$a = \begin{bmatrix} 0.8 \\ 0.1 \\ 0.1 \end{bmatrix}$ is <b>more desirable</b> than having
$a = \begin{bmatrix} 0.6 \\ 0.2 \\ 0.2 \end{bmatrix}$.

<p>
In other words, we want to <b>maximize</b>:
</p>

$$P(y|x) = \prod_{j=1}^{3} {a_j^{y_j} (1-a_j)^{(1-y_j)} }$$

<p>
Here, $a_j$ represents the jth item in the vector $a$, and similarly $y_j$ represents the jth value in $y$.
For example, when $a = \begin{bmatrix} 0.9 \\ 0.1 \\ 0.0 \end{bmatrix}$, then, $a_1 = 0.9, a_2 = 0.1$ and
$a_3 = 0.0$.
</p>

$$y = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, a = \begin{bmatrix} 0.9 \\ 0.1 \\ 0.0 \end{bmatrix}, P(y|x) = 0.9 \times 0.9 \times 1.0 = 0.810 $$

$$y = \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}, a = \begin{bmatrix} 0.1 \\ 0.8 \\ 0.1 \end{bmatrix}, P(y|x) = 0.9 \times 0.8 \times 0.9 = 0.648 $$

$$y = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}, a = \begin{bmatrix} 0.1 \\ 0.2 \\ 0.7 \end{bmatrix}, P(y|x) = 0.9 \times 0.8 \times 0.7 = 0.504 $$

<p>
<b>Bigger</b> the $P(y|x)$ is the <b>better</b>.
</p>

<p>
Similar to <b>Logistic Regression</b>, in order to define the <b>loss</b> for multiple samples, we will simply multiply
each value for each sample (<b>Maximum Likelihood Estimation</b>):
</p>

$$J = \prod_{i=1}^{M} P(y^{(i)}|x^{(i)}) $$

<p>
here, $y^{(i)}$, $x^{(i)}$ and $a^{(i)}$ corresponds to ith sample in the training set out of $M$ training samples. We can rewrite it as:
</p>

$$J = \prod_{i=1}^{M} \prod_{j=1}^{3}  (a_j^{(i)}) ^ {y_j^{(i)}} (1-a_j^{(i)})^{(1-y_j^{(i)})} $$

<p>
Maximizing above is equal to maximizing:
</p>

$$J = log \left( \prod_{i=1}^{M} \prod_{j=1}^{3} (a_j^{(i)}) ^ {y_j^{(i)}} (1-a_j^{(i)})^{(1-y_j^{(i)})} \right ) $$

<p>
with more math:
</p>

$$J = \sum_{i=1}^{M} \sum_{j=1}^{3} y_j^{(i)} log(a_j^{(i)}) (1-y_j^{(i)}) log(1-a_j^{(i)}) $$

<p>
since we like to <b>minimize</b> things instead of <b>maximizing</b>:
</p>


$$J = - \sum_{i=1}^{M} \sum_{j=1}^{3} y_j^{(i)} log(a_j^{(i)}) (1-y_j^{(i)}) log(1-a_j^{(i)}) $$

<p>
If we add our <b>Log Loss</b> to our computation graph:
</p>

<img class="static_image" style="width: 800px;" src="static_images/image004.png"/>


<h1>Gradient Descent</h1>

<p>
Let's see what happens if we change $w_{2, 1}$ in the network, step by step:
</p>

<img class="static_image" style="width: 900px;" src="static_images/image005.gif"/>

<p>
In order to do gradient descent, we need the derivatives:
</p>

$$
\frac{dL}{dw_{m,n}} = \sum_{i=1}^3 \left (\frac{dL}{da_i} \right) \left (\frac{da_i}{dz'_m} \right)  \left (\frac{dz'_m}{dz_m} \right ) \left(\frac{dz_m}{dw_{m,n}} \right ), \quad

\frac{dL}{db} = \sum_{i=1}^3 \left (\frac{dL}{da_i} \right) \left (\frac{da_i}{dz'_m} \right)  \left (\frac{dz'_m}{dz_m} \right ) \left(\frac{dz_m}{b_m} \right )
$$

<p>
Let's do some calculus:
<img src="static_images/evil1.png" style="width:30px; height:30px;">
<img src="static_images/evil2.png" style="width:30px; height:30px;">
</p>

$$
\frac{dL}{da_i} =  \frac{d}{da_i} \left ( - \sum_{j=1}^3 y_j log(a_j) + (1-y_j) log(1-a_j) \right ) =
\frac{d}{da_i} \left ( - y_i log(a_i) \right ) + \frac{d}{da_i} \left ( - (1-y_i) log(1-a_i) \right ) =
\frac{-y_i}{a_i} + \frac{(1-y_i)}{1-a_i}
$$

<h1>Derivative of Softmax Function</h1>

<p>
The Softmax function takes an N-dimensional vector of real values and
returns a new N-dimensional vector that sums up to $1$. The exact formula is (Softmax equation):
</p>


<p class="equation">
\begin{equation} \label{eq:softmax}
a_i = \frac{e^{z'_i}}{\sum_{j=1}^N e^{z'_j}}
\end{equation}
</p>

<p>
Let's make an example:
</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span>
<span class="k">print</span> <span class="n">a</span>
<span class="k">print</span> <span class="n">softmax</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</pre></div>
<div class="code_stdout"><pre>[ 1.  2.  3.]
[ 0.09003057  0.24472847  0.66524096]
</pre></div><p>
Intuitively, softmax increases/emphasizes the <b>relative difference</b> between large and small values.
</p>

<p>
A nice property about the Softmax function that it produces a legit <b>Probability Distrubition</b>.
In classification (or more generally in Machine Learning) we often want to assign prababilities to
categories or classes. Softmax function is known to work well numereous applications/areas.
</p>

<p>
Softmax is a vector function -- it takes a vector as an input and returns another vector. Therefore, we
cannot just ask for <b>the derivative of softmax</b>, we can only ask the derivative of softmax regarding
particular elements.
</p>

<p>
For example,
</p>

$$
\frac{d}{d z'_2} a_1
$$

<p>
refers to how much $a_1$ will change if play with $z'_2$.
</p>

<p>
Using the same logic for each element for $a_i$ and $z'_j$ would produce us $N \times N$ matrix of derivatives.
</p>

<p>
Let's try to take derivative for <b>one particular element</b>:
</p>

$$
\frac{d}{d z'_m} a_i = \frac{d}{d z'_m} \frac{e^{z'_i}}{\sum_{j=1}^N e^{z'_j}}
$$

<p>
We can use <b>Quotient Rule</b> here. Recall that:
</p>

$$
\frac{d}{dx} \frac{f(x)}{g(x)} = \frac{f'(x)g(x) - g'(x)f(x)}{ [g(x)]^2 }
$$

<p>
In our case:
</p>

$$
f(x) = e^{z'_i}, \quad g(x) = \sum_{j=1}^N e^{z'_j}
$$

<p>
Let's apply <b>Quotient Rule</b>, if $m=n$:
</p>

$$
\frac{d}{d z'_m} a_i = \frac{d}{d z'_m} \frac{e^{z_i}}{\sum_{j=1}^N e^{z'_j}} =
\frac{ (e^{z_m})' \sum_{j=1}^N e^{z_j}  - (\sum_{j=1}^N e^{z_j})' e^{z_m} }{ [\sum_{j=1}^N e^{z_j}] ^ 2 } =
\frac{ e^{z_m} \sum_{j=1}^N e^{z_j} - e^{z_m} e^{z_n} }{ [\sum_{j=1}^N e^{z_j}] ^ 2 } =
\frac{ e^{z_m} } { \sum_{j=1}^N e^{z_j} }  \frac{ \sum_{j=1}^N - e^{z_n} } { \sum_{j=1}^N e^{z_j} } = (a_m)(1-a_n)
$$

<p>
Notice that we simplify, by plugging $a_i$ (see Equation \ref{eq:softmax}) in the last step above.
</p>

<p>
Similarly, if $m \neq n$:
</p>

$$
\frac{d}{d z_n} a_m = \frac{d}{d z_n} \frac{e^{z_m}}{\sum_{j=1}^N e^{z_j}} =
\frac{ (e^{z_m})' \sum_{j=1}^N e^{z_j}  - (\sum_{j=1}^N e^{z_j})' e^{z_m} }{ [\sum_{j=1}^N e^{z_j}] ^ 2 } =
\frac{ 0 - e^{z_m} e^{z_n} }{ [\sum_{j=1}^N e^{z_j}] ^ 2 } = -\frac{ e^{z_m} } { \sum_{j=1}^N e^{z_j} } \frac{ e^{z_n} }{ \sum_{j=1}^N e^{z_j} }
= - (a_m) (a_n)
$$

<p>
More succintly, we can summarize all above:
</p>

$$
\frac{d}{d z_n} a_m =
  \begin{cases}
    (a_m)(1-a_n), & \text{if}\ \quad  m = n \\
    - (a_m) (a_n), & \text{if}\ \quad  m \neq n
  \end{cases}
$$

<p>
We know the derivative of sigmoid function from the previous lecture:
</p>

$$
\frac{dz'}{dz} = g(z)(1-g(z))
$$

<p>
Similarly, we also calculated (and easy to see):
</p>

$$
\frac{dz}{dw_{m,n}} = x_n, \quad \frac{dz}{db_i} = 1
$$

<p>
Now, let's combine these:
</p>


<h1>References</h1>

<ul>
  <li> http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/
  <li> https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/ </li>
  <li> http://tutorial.math.lamar.edu/Classes/CalcI/ProductQuotientRule.aspx </li>
</ul>
    </div>
</div>

</body>
</html>
